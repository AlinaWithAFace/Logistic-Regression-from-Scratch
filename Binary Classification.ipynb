{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression from scratch- For Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.matlib\n",
    "from sklearn.metrics import accuracy_score\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data and training label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataset- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = [2,3]# Cluster center for class 1 data\n",
    "c2 = [10,11]# Cluster center for class 2 data\n",
    "no = 50 # No of samples in a class\n",
    "class1 = np.matlib.repmat(c1,no,1) + np.random.randn(no,len(c1))\n",
    "class2 = np.matlib.repmat(c2, no,1)+ np.random.randn(no,len(c2))\n",
    "D = np.append(class1,class2,axis =0)\n",
    "Data = np.concatenate((D, np.ones((2*no,1))),axis = 1)\n",
    "c1_label = np.ones((no,1))\n",
    "c2_label = -1*np.ones((no,1))\n",
    "label = np.concatenate((c1_label,c2_label),axis = 0)\n",
    "Data = Data.T\n",
    "y = label.T# True label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIEg0AEgEP8PSulfQE09DcEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27fbed2e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(class1[:,0],class1[:,1],'ro',class2[:,0],class2[:,1],'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = [5,4]# Cluster center for class1 data of validation data set\n",
    "v2 = [7,12]# Cluster center for class 2 data of validation data set\n",
    "v_no = 30# No of samples in a class\n",
    "v_class1 = np.matlib.repmat(v1,v_no,1) + np.random.randn(v_no,len(v1))\n",
    "v_class2 = np.matlib.repmat(v2, v_no,1)+ np.random.randn(v_no,len(v2))\n",
    "v_D = np.append(v_class1,v_class2,axis =0)\n",
    "v_Data = np.concatenate((v_D, np.ones((2*v_no,1))),axis = 1)\n",
    "v_c1_label = np.ones((v_no,1))\n",
    "v_c2_label = -1*np.ones((v_no,1))\n",
    "v_label = np.concatenate((v_c1_label,v_c2_label),axis = 0)\n",
    "v_Data = v_Data.T\n",
    "v_y = v_label.T# Test label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27f97e9c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(v_class1[:,0],v_class1[:,1],'ro',class2[:,0],class2[:,1],'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The activation function used is sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    a  = 1/(1+np.exp(-x))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, Data):\n",
    "    pred = []\n",
    "    z = np.dot(w,Data)\n",
    "    a = sigmoid(z)\n",
    "    for i in range(0,len(a[0])):\n",
    "        if (a[0][i] > 0.5): \n",
    "            pred.append(1)\n",
    "        elif (a[0][i] <= 0.5):\n",
    "            pred.append(-1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\lr.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part- In this step the loss is calculated and minimized with respect to weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1, 'Loss', 89.106166131671159, 'Training Accuracy', 55.000000000000007)\n",
      "('Epoch', 2, 'Loss', 184.07926061654669, 'Training Accuracy', 39.0)\n",
      "('Epoch', 3, 'Loss', 1158.1302902473956, 'Training Accuracy', 50.0)\n",
      "('Epoch', 4, 'Loss', 783.34390369741618, 'Training Accuracy', 50.0)\n",
      "('Epoch', 5, 'Loss', 409.29567542475337, 'Training Accuracy', 50.0)\n",
      "('Epoch', 6, 'Loss', 63.12546183047855, 'Training Accuracy', 63.0)\n",
      "('Epoch', 7, 'Loss', 401.74198484960993, 'Training Accuracy', 50.0)\n",
      "('Epoch', 8, 'Loss', 1181.6299694865643, 'Training Accuracy', 50.0)\n",
      "('Epoch', 9, 'Loss', 807.43100887582216, 'Training Accuracy', 50.0)\n",
      "('Epoch', 10, 'Loss', 436.52336017942537, 'Training Accuracy', 51.0)\n",
      "('Epoch', 11, 'Loss', 107.56829182143922, 'Training Accuracy', 64.0)\n",
      "('Epoch', 12, 'Loss', 28.889017708049497, 'Training Accuracy', 88.0)\n",
      "('Epoch', 13, 'Loss', 132.37495321296257, 'Training Accuracy', 62.0)\n",
      "('Epoch', 14, 'Loss', 15.714082033856641, 'Training Accuracy', 93.0)\n",
      "('Epoch', 15, 'Loss', 31.776277043651113, 'Training Accuracy', 84.0)\n",
      "('Epoch', 16, 'Loss', 22.40718693292467, 'Training Accuracy', 91.0)\n",
      "('Epoch', 17, 'Loss', 89.538169934162781, 'Training Accuracy', 70.0)\n",
      "('Epoch', 18, 'Loss', 16.606184650974395, 'Training Accuracy', 93.0)\n",
      "('Epoch', 19, 'Loss', 48.427079427886987, 'Training Accuracy', 78.0)\n",
      "('Epoch', 20, 'Loss', 15.427553016031055, 'Training Accuracy', 93.0)\n",
      "('Epoch', 21, 'Loss', 45.002783963376316, 'Training Accuracy', 79.0)\n",
      "('Epoch', 22, 'Loss', 12.592802314085892, 'Training Accuracy', 96.0)\n",
      "('Epoch', 23, 'Loss', 30.226107237968534, 'Training Accuracy', 84.0)\n",
      "('Epoch', 24, 'Loss', 9.4698346529957416, 'Training Accuracy', 98.0)\n",
      "('Epoch', 25, 'Loss', 16.270151809008791, 'Training Accuracy', 93.0)\n",
      "('Epoch', 26, 'Loss', 7.3153173963358764, 'Training Accuracy', 99.0)\n",
      "('Epoch', 27, 'Loss', 9.3249225488070859, 'Training Accuracy', 97.0)\n",
      "('Epoch', 28, 'Loss', 6.5185227866604727, 'Training Accuracy', 99.0)\n",
      "('Epoch', 29, 'Loss', 7.7122605051708204, 'Training Accuracy', 97.0)\n",
      "('Epoch', 30, 'Loss', 5.9236209365770147, 'Training Accuracy', 99.0)\n",
      "('Epoch', 31, 'Loss', 6.673799148799052, 'Training Accuracy', 98.0)\n",
      "('Epoch', 32, 'Loss', 5.4349835107521276, 'Training Accuracy', 99.0)\n",
      "('Epoch', 33, 'Loss', 5.8897736141665735, 'Training Accuracy', 98.0)\n",
      "('Epoch', 34, 'Loss', 5.0286622415635369, 'Training Accuracy', 99.0)\n",
      "('Epoch', 35, 'Loss', 5.2850405123475994, 'Training Accuracy', 100.0)\n",
      "('Epoch', 36, 'Loss', 4.6872213807136553, 'Training Accuracy', 99.0)\n",
      "('Epoch', 37, 'Loss', 4.8111648830549987, 'Training Accuracy', 100.0)\n",
      "('Epoch', 38, 'Loss', 4.3979355892904772, 'Training Accuracy', 100.0)\n",
      "('Epoch', 39, 'Loss', 4.4358605567463378, 'Training Accuracy', 100.0)\n",
      "('Epoch', 40, 'Loss', 4.1514073063876484, 'Training Accuracy', 100.0)\n",
      "('Epoch', 41, 'Loss', 4.1365789042621213, 'Training Accuracy', 100.0)\n",
      "('Epoch', 42, 'Loss', 3.9406388880790248, 'Training Accuracy', 100.0)\n",
      "('Epoch', 43, 'Loss', 3.8968300562885205, 'Training Accuracy', 100.0)\n",
      "('Epoch', 44, 'Loss', 3.7602891543556125, 'Training Accuracy', 100.0)\n",
      "('Epoch', 45, 'Loss', 3.7038888810624675, 'Training Accuracy', 100.0)\n",
      "('Epoch', 46, 'Loss', 3.606003231774122, 'Training Accuracy', 100.0)\n",
      "('Epoch', 47, 'Loss', 3.547355652491373, 'Training Accuracy', 100.0)\n",
      "('Epoch', 48, 'Loss', 3.4738325091376381, 'Training Accuracy', 100.0)\n",
      "('Epoch', 49, 'Loss', 3.4184027061674422, 'Training Accuracy', 100.0)\n",
      "('Epoch', 50, 'Loss', 3.3598814175946368, 'Training Accuracy', 100.0)\n",
      "('Epoch', 51, 'Loss', 3.3096066230529595, 'Training Accuracy', 100.0)\n",
      "('Epoch', 52, 'Loss', 3.2603265226135481, 'Training Accuracy', 100.0)\n",
      "('Epoch', 53, 'Loss', 3.2151165707443492, 'Training Accuracy', 100.0)\n",
      "('Epoch', 54, 'Loss', 3.171753345030373, 'Training Accuracy', 100.0)\n",
      "('Epoch', 55, 'Loss', 3.1307723797362499, 'Training Accuracy', 100.0)\n",
      "('Epoch', 56, 'Loss', 3.0915063711258814, 'Training Accuracy', 100.0)\n",
      "('Epoch', 57, 'Loss', 3.0539169575366669, 'Training Accuracy', 100.0)\n",
      "('Epoch', 58, 'Loss', 3.0177702728846474, 'Training Accuracy', 100.0)\n",
      "('Epoch', 59, 'Loss', 2.9829638650115786, 'Training Accuracy', 100.0)\n",
      "('Epoch', 60, 'Loss', 2.9493849049428138, 'Training Accuracy', 100.0)\n",
      "('Epoch', 61, 'Loss', 2.9169506405082757, 'Training Accuracy', 100.0)\n",
      "('Epoch', 62, 'Loss', 2.8855872536063498, 'Training Accuracy', 100.0)\n",
      "('Epoch', 63, 'Loss', 2.8552306572008552, 'Training Accuracy', 100.0)\n",
      "('Epoch', 64, 'Loss', 2.8258233401792614, 'Training Accuracy', 100.0)\n",
      "('Epoch', 65, 'Loss', 2.7973127664019533, 'Training Accuracy', 100.0)\n",
      "('Epoch', 66, 'Loss', 2.7696507227500455, 'Training Accuracy', 100.0)\n",
      "('Epoch', 67, 'Loss', 2.7427925012542147, 'Training Accuracy', 100.0)\n",
      "('Epoch', 68, 'Loss', 2.7166966213759989, 'Training Accuracy', 100.0)\n",
      "('Epoch', 69, 'Loss', 2.691324434342564, 'Training Accuracy', 100.0)\n",
      "('Epoch', 70, 'Loss', 2.6666399164803289, 'Training Accuracy', 100.0)\n",
      "('Epoch', 71, 'Loss', 2.6426094279307466, 'Training Accuracy', 100.0)\n",
      "('Epoch', 72, 'Loss', 2.6192015354832012, 'Training Accuracy', 100.0)\n",
      "('Epoch', 73, 'Loss', 2.596386834234194, 'Training Accuracy', 100.0)\n",
      "('Epoch', 74, 'Loss', 2.5741377956342864, 'Training Accuracy', 100.0)\n",
      "('Epoch', 75, 'Loss', 2.552428623932542, 'Training Accuracy', 100.0)\n",
      "('Epoch', 76, 'Loss', 2.531235127438447, 'Training Accuracy', 100.0)\n",
      "('Epoch', 77, 'Loss', 2.5105345994685444, 'Training Accuracy', 100.0)\n",
      "('Epoch', 78, 'Loss', 2.4903057098132493, 'Training Accuracy', 100.0)\n",
      "('Epoch', 79, 'Loss', 2.4705284049168212, 'Training Accuracy', 100.0)\n",
      "('Epoch', 80, 'Loss', 2.4511838163576436, 'Training Accuracy', 100.0)\n",
      "('Epoch', 81, 'Loss', 2.432254176704852, 'Training Accuracy', 100.0)\n",
      "('Epoch', 82, 'Loss', 2.4137227421672525, 'Training Accuracy', 100.0)\n",
      "('Epoch', 83, 'Loss', 2.3955737213979695, 'Training Accuracy', 100.0)\n",
      "('Epoch', 84, 'Loss', 2.3777922099268642, 'Training Accuracy', 100.0)\n",
      "('Epoch', 85, 'Loss', 2.3603641297233762, 'Training Accuracy', 100.0)\n",
      "('Epoch', 86, 'Loss', 2.343276173447939, 'Training Accuracy', 100.0)\n",
      "('Epoch', 87, 'Loss', 2.326515752988418, 'Training Accuracy', 100.0)\n",
      "('Epoch', 88, 'Loss', 2.310070951917083, 'Training Accuracy', 100.0)\n",
      "('Epoch', 89, 'Loss', 2.2939304815369721, 'Training Accuracy', 100.0)\n",
      "('Epoch', 90, 'Loss', 2.2780836402171647, 'Training Accuracy', 100.0)\n",
      "('Epoch', 91, 'Loss', 2.2625202757438467, 'Training Accuracy', 100.0)\n",
      "('Epoch', 92, 'Loss', 2.2472307504388889, 'Training Accuracy', 100.0)\n",
      "('Epoch', 93, 'Loss', 2.2322059088199442, 'Training Accuracy', 100.0)\n",
      "('Epoch', 94, 'Loss', 2.2174370475962499, 'Training Accuracy', 100.0)\n",
      "('Epoch', 95, 'Loss', 2.2029158878124973, 'Training Accuracy', 100.0)\n",
      "('Epoch', 96, 'Loss', 2.1886345489697385, 'Training Accuracy', 100.0)\n",
      "('Epoch', 97, 'Loss', 2.1745855249670321, 'Training Accuracy', 100.0)\n",
      "('Epoch', 98, 'Loss', 2.1607616617212013, 'Training Accuracy', 100.0)\n",
      "('Epoch', 99, 'Loss', 2.14715613633423, 'Training Accuracy', 100.0)\n",
      "('Epoch', 100, 'Loss', 2.1337624376889286, 'Training Accuracy', 100.0)\n",
      "('Epoch', 101, 'Loss', 2.1205743483636526, 'Training Accuracy', 100.0)\n",
      "('Epoch', 102, 'Loss', 2.1075859277659297, 'Training Accuracy', 100.0)\n",
      "('Epoch', 103, 'Loss', 2.0947914963932406, 'Training Accuracy', 100.0)\n",
      "('Epoch', 104, 'Loss', 2.0821856211367509, 'Training Accuracy', 100.0)\n",
      "('Epoch', 105, 'Loss', 2.0697631015506874, 'Training Accuracy', 100.0)\n",
      "('Epoch', 106, 'Loss', 2.0575189570163297, 'Training Accuracy', 100.0)\n",
      "('Epoch', 107, 'Loss', 2.0454484147353806, 'Training Accuracy', 100.0)\n",
      "('Epoch', 108, 'Loss', 2.0335468984925975, 'Training Accuracy', 100.0)\n",
      "('Epoch', 109, 'Loss', 2.0218100181324332, 'Training Accuracy', 100.0)\n",
      "('Epoch', 110, 'Loss', 2.0102335596987766, 'Training Accuracy', 100.0)\n",
      "('Epoch', 111, 'Loss', 1.9988134761908538, 'Training Accuracy', 100.0)\n",
      "('Epoch', 112, 'Loss', 1.9875458788919862, 'Training Accuracy', 100.0)\n",
      "('Epoch', 113, 'Loss', 1.976427029231308, 'Training Accuracy', 100.0)\n",
      "('Epoch', 114, 'Loss', 1.9654533311415434, 'Training Accuracy', 100.0)\n",
      "('Epoch', 115, 'Loss', 1.9546213238787622, 'Training Accuracy', 100.0)\n",
      "('Epoch', 116, 'Loss', 1.9439276752726415, 'Training Accuracy', 100.0)\n",
      "('Epoch', 117, 'Loss', 1.9333691753780784, 'Training Accuracy', 100.0)\n",
      "('Epoch', 118, 'Loss', 1.9229427305012077, 'Training Accuracy', 100.0)\n",
      "('Epoch', 119, 'Loss', 1.9126453575748446, 'Training Accuracy', 100.0)\n",
      "('Epoch', 120, 'Loss', 1.9024741788602086, 'Training Accuracy', 100.0)\n",
      "('Epoch', 121, 'Loss', 1.8924264169535223, 'Training Accuracy', 100.0)\n",
      "('Epoch', 122, 'Loss', 1.8824993900775386, 'Training Accuracy', 100.0)\n",
      "('Epoch', 123, 'Loss', 1.8726905076395861, 'Training Accuracy', 100.0)\n",
      "('Epoch', 124, 'Loss', 1.8629972660389378, 'Training Accuracy', 100.0)\n",
      "('Epoch', 125, 'Loss', 1.8534172447076511, 'Training Accuracy', 100.0)\n",
      "('Epoch', 126, 'Loss', 1.8439481023699962, 'Training Accuracy', 100.0)\n",
      "('Epoch', 127, 'Loss', 1.8345875735067734, 'Training Accuracy', 100.0)\n",
      "('Epoch', 128, 'Loss', 1.8253334650116779, 'Training Accuracy', 100.0)\n",
      "('Epoch', 129, 'Loss', 1.8161836530278059, 'Training Accuracy', 100.0)\n",
      "('Epoch', 130, 'Loss', 1.807136079953195, 'Training Accuracy', 100.0)\n",
      "('Epoch', 131, 'Loss', 1.7981887516050421, 'Training Accuracy', 100.0)\n",
      "('Epoch', 132, 'Loss', 1.7893397345329907, 'Training Accuracy', 100.0)\n",
      "('Epoch', 133, 'Loss', 1.7805871534724487, 'Training Accuracy', 100.0)\n",
      "('Epoch', 134, 'Loss', 1.7719291889295692, 'Training Accuracy', 100.0)\n",
      "('Epoch', 135, 'Loss', 1.7633640748900632, 'Training Accuracy', 100.0)\n",
      "('Epoch', 136, 'Loss', 1.7548900966445207, 'Training Accuracy', 100.0)\n",
      "('Epoch', 137, 'Loss', 1.7465055887233802, 'Training Accuracy', 100.0)\n",
      "('Epoch', 138, 'Loss', 1.7382089329352224, 'Training Accuracy', 100.0)\n",
      "('Epoch', 139, 'Loss', 1.7299985565023219, 'Training Accuracy', 100.0)\n",
      "('Epoch', 140, 'Loss', 1.7218729302879405, 'Training Accuracy', 100.0)\n",
      "('Epoch', 141, 'Loss', 1.7138305671100831, 'Training Accuracy', 100.0)\n",
      "('Epoch', 142, 'Loss', 1.705870020136854, 'Training Accuracy', 100.0)\n",
      "('Epoch', 143, 'Loss', 1.6979898813587657, 'Training Accuracy', 100.0)\n",
      "('Epoch', 144, 'Loss', 1.6901887801337712, 'Training Accuracy', 100.0)\n",
      "('Epoch', 145, 'Loss', 1.6824653818009165, 'Training Accuracy', 100.0)\n",
      "('Epoch', 146, 'Loss', 1.6748183863588697, 'Training Accuracy', 100.0)\n",
      "('Epoch', 147, 'Loss', 1.6672465272057753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 148, 'Loss', 1.6597485699370791, 'Training Accuracy', 100.0)\n",
      "('Epoch', 149, 'Loss', 1.6523233111982183, 'Training Accuracy', 100.0)\n",
      "('Epoch', 150, 'Loss', 1.6449695775892126, 'Training Accuracy', 100.0)\n",
      "('Epoch', 151, 'Loss', 1.637686224618415, 'Training Accuracy', 100.0)\n",
      "('Epoch', 152, 'Loss', 1.630472135702818, 'Training Accuracy', 100.0)\n",
      "('Epoch', 153, 'Loss', 1.6233262212124462, 'Training Accuracy', 100.0)\n",
      "('Epoch', 154, 'Loss', 1.616247417556588, 'Training Accuracy', 100.0)\n",
      "('Epoch', 155, 'Loss', 1.6092346863096536, 'Training Accuracy', 100.0)\n",
      "('Epoch', 156, 'Loss', 1.6022870133746339, 'Training Accuracy', 100.0)\n",
      "('Epoch', 157, 'Loss', 1.5954034081822659, 'Training Accuracy', 100.0)\n",
      "('Epoch', 158, 'Loss', 1.5885829029240537, 'Training Accuracy', 100.0)\n",
      "('Epoch', 159, 'Loss', 1.5818245518174874, 'Training Accuracy', 100.0)\n",
      "('Epoch', 160, 'Loss', 1.5751274304017913, 'Training Accuracy', 100.0)\n",
      "('Epoch', 161, 'Loss', 1.568490634862751, 'Training Accuracy', 100.0)\n",
      "('Epoch', 162, 'Loss', 1.5619132813851409, 'Training Accuracy', 100.0)\n",
      "('Epoch', 163, 'Loss', 1.5553945055314129, 'Training Accuracy', 100.0)\n",
      "('Epoch', 164, 'Loss', 1.5489334616453763, 'Training Accuracy', 100.0)\n",
      "('Epoch', 165, 'Loss', 1.5425293222796619, 'Training Accuracy', 100.0)\n",
      "('Epoch', 166, 'Loss', 1.5361812776458166, 'Training Accuracy', 100.0)\n",
      "('Epoch', 167, 'Loss', 1.5298885350859674, 'Training Accuracy', 100.0)\n",
      "('Epoch', 168, 'Loss', 1.5236503185650387, 'Training Accuracy', 100.0)\n",
      "('Epoch', 169, 'Loss', 1.5174658681825337, 'Training Accuracy', 100.0)\n",
      "('Epoch', 170, 'Loss', 1.5113344397029944, 'Training Accuracy', 100.0)\n",
      "('Epoch', 171, 'Loss', 1.5052553041042673, 'Training Accuracy', 100.0)\n",
      "('Epoch', 172, 'Loss', 1.4992277471427471, 'Training Accuracy', 100.0)\n",
      "('Epoch', 173, 'Loss', 1.4932510689348426, 'Training Accuracy', 100.0)\n",
      "('Epoch', 174, 'Loss', 1.4873245835539219, 'Training Accuracy', 100.0)\n",
      "('Epoch', 175, 'Loss', 1.4814476186420225, 'Training Accuracy', 100.0)\n",
      "('Epoch', 176, 'Loss', 1.4756195150357057, 'Training Accuracy', 100.0)\n",
      "('Epoch', 177, 'Loss', 1.4698396264053848, 'Training Accuracy', 100.0)\n",
      "('Epoch', 178, 'Loss', 1.4641073189075799, 'Training Accuracy', 100.0)\n",
      "('Epoch', 179, 'Loss', 1.4584219708494919, 'Training Accuracy', 100.0)\n",
      "('Epoch', 180, 'Loss', 1.452782972365378, 'Training Accuracy', 100.0)\n",
      "('Epoch', 181, 'Loss', 1.4471897251042349, 'Training Accuracy', 100.0)\n",
      "('Epoch', 182, 'Loss', 1.4416416419282887, 'Training Accuracy', 100.0)\n",
      "('Epoch', 183, 'Loss', 1.4361381466218215, 'Training Accuracy', 100.0)\n",
      "('Epoch', 184, 'Loss', 1.4306786736099391, 'Training Accuracy', 100.0)\n",
      "('Epoch', 185, 'Loss', 1.4252626676868165, 'Training Accuracy', 100.0)\n",
      "('Epoch', 186, 'Loss', 1.419889583753053, 'Training Accuracy', 100.0)\n",
      "('Epoch', 187, 'Loss', 1.4145588865617709, 'Training Accuracy', 100.0)\n",
      "('Epoch', 188, 'Loss', 1.4092700504730653, 'Training Accuracy', 100.0)\n",
      "('Epoch', 189, 'Loss', 1.4040225592164957, 'Training Accuracy', 100.0)\n",
      "('Epoch', 190, 'Loss', 1.3988159056612839, 'Training Accuracy', 100.0)\n",
      "('Epoch', 191, 'Loss', 1.3936495915939051, 'Training Accuracy', 100.0)\n",
      "('Epoch', 192, 'Loss', 1.3885231275027796, 'Training Accuracy', 100.0)\n",
      "('Epoch', 193, 'Loss', 1.3834360323697981, 'Training Accuracy', 100.0)\n",
      "('Epoch', 194, 'Loss', 1.3783878334683792, 'Training Accuracy', 100.0)\n",
      "('Epoch', 195, 'Loss', 1.3733780661678439, 'Training Accuracy', 100.0)\n",
      "('Epoch', 196, 'Loss', 1.368406273743846, 'Training Accuracy', 100.0)\n",
      "('Epoch', 197, 'Loss', 1.3634720071945949, 'Training Accuracy', 100.0)\n",
      "('Epoch', 198, 'Loss', 1.3585748250627232, 'Training Accuracy', 100.0)\n",
      "('Epoch', 199, 'Loss', 1.353714293262491, 'Training Accuracy', 100.0)\n",
      "('Epoch', 200, 'Loss', 1.3488899849122276, 'Training Accuracy', 100.0)\n",
      "('Epoch', 201, 'Loss', 1.3441014801717095, 'Training Accuracy', 100.0)\n",
      "('Epoch', 202, 'Loss', 1.3393483660843868, 'Training Accuracy', 100.0)\n",
      "('Epoch', 203, 'Loss', 1.334630236424224, 'Training Accuracy', 100.0)\n",
      "('Epoch', 204, 'Loss', 1.329946691546984, 'Training Accuracy', 100.0)\n",
      "('Epoch', 205, 'Loss', 1.3252973382458364, 'Training Accuracy', 100.0)\n",
      "('Epoch', 206, 'Loss', 1.3206817896110923, 'Training Accuracy', 100.0)\n",
      "('Epoch', 207, 'Loss', 1.3160996648939209, 'Training Accuracy', 100.0)\n",
      "('Epoch', 208, 'Loss', 1.3115505893739556, 'Training Accuracy', 100.0)\n",
      "('Epoch', 209, 'Loss', 1.3070341942305741, 'Training Accuracy', 100.0)\n",
      "('Epoch', 210, 'Loss', 1.3025501164177886, 'Training Accuracy', 100.0)\n",
      "('Epoch', 211, 'Loss', 1.2980979985425913, 'Training Accuracy', 100.0)\n",
      "('Epoch', 212, 'Loss', 1.2936774887466371, 'Training Accuracy', 100.0)\n",
      "('Epoch', 213, 'Loss', 1.2892882405911501, 'Training Accuracy', 100.0)\n",
      "('Epoch', 214, 'Loss', 1.2849299129449576, 'Training Accuracy', 100.0)\n",
      "('Epoch', 215, 'Loss', 1.2806021698755246, 'Training Accuracy', 100.0)\n",
      "('Epoch', 216, 'Loss', 1.2763046805428906, 'Training Accuracy', 100.0)\n",
      "('Epoch', 217, 'Loss', 1.2720371190964475, 'Training Accuracy', 100.0)\n",
      "('Epoch', 218, 'Loss', 1.2677991645743918, 'Training Accuracy', 100.0)\n",
      "('Epoch', 219, 'Loss', 1.2635905008058455, 'Training Accuracy', 100.0)\n",
      "('Epoch', 220, 'Loss', 1.2594108163155016, 'Training Accuracy', 100.0)\n",
      "('Epoch', 221, 'Loss', 1.2552598042307215, 'Training Accuracy', 100.0)\n",
      "('Epoch', 222, 'Loss', 1.2511371621910388, 'Training Accuracy', 100.0)\n",
      "('Epoch', 223, 'Loss', 1.2470425922599351, 'Training Accuracy', 100.0)\n",
      "('Epoch', 224, 'Loss', 1.242975800838878, 'Training Accuracy', 100.0)\n",
      "('Epoch', 225, 'Loss', 1.2389364985834914, 'Training Accuracy', 100.0)\n",
      "('Epoch', 226, 'Loss', 1.2349244003218356, 'Training Accuracy', 100.0)\n",
      "('Epoch', 227, 'Loss', 1.2309392249747162, 'Training Accuracy', 100.0)\n",
      "('Epoch', 228, 'Loss', 1.2269806954779341, 'Training Accuracy', 100.0)\n",
      "('Epoch', 229, 'Loss', 1.2230485387064676, 'Training Accuracy', 100.0)\n",
      "('Epoch', 230, 'Loss', 1.2191424854004798, 'Training Accuracy', 100.0)\n",
      "('Epoch', 231, 'Loss', 1.2152622700931188, 'Training Accuracy', 100.0)\n",
      "('Epoch', 232, 'Loss', 1.2114076310400628, 'Training Accuracy', 100.0)\n",
      "('Epoch', 233, 'Loss', 1.2075783101507309, 'Training Accuracy', 100.0)\n",
      "('Epoch', 234, 'Loss', 1.2037740529211371, 'Training Accuracy', 100.0)\n",
      "('Epoch', 235, 'Loss', 1.1999946083683324, 'Training Accuracy', 100.0)\n",
      "('Epoch', 236, 'Loss', 1.1962397289663644, 'Training Accuracy', 100.0)\n",
      "('Epoch', 237, 'Loss', 1.1925091705837534, 'Training Accuracy', 100.0)\n",
      "('Epoch', 238, 'Loss', 1.188802692422404, 'Training Accuracy', 100.0)\n",
      "('Epoch', 239, 'Loss', 1.185120056957913, 'Training Accuracy', 100.0)\n",
      "('Epoch', 240, 'Loss', 1.1814610298812753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 241, 'Loss', 1.1778253800418839, 'Training Accuracy', 100.0)\n",
      "('Epoch', 242, 'Loss', 1.1742128793918365, 'Training Accuracy', 100.0)\n",
      "('Epoch', 243, 'Loss', 1.1706233029314894, 'Training Accuracy', 100.0)\n",
      "('Epoch', 244, 'Loss', 1.1670564286562393, 'Training Accuracy', 100.0)\n",
      "('Epoch', 245, 'Loss', 1.1635120375044723, 'Training Accuracy', 100.0)\n",
      "('Epoch', 246, 'Loss', 1.1599899133066818, 'Training Accuracy', 100.0)\n",
      "('Epoch', 247, 'Loss', 1.1564898427356904, 'Training Accuracy', 100.0)\n",
      "('Epoch', 248, 'Loss', 1.1530116152579719, 'Training Accuracy', 100.0)\n",
      "('Epoch', 249, 'Loss', 1.1495550230860263, 'Training Accuracy', 100.0)\n",
      "('Epoch', 250, 'Loss', 1.1461198611317946, 'Training Accuracy', 100.0)\n",
      "('Epoch', 251, 'Loss', 1.1427059269610571, 'Training Accuracy', 100.0)\n",
      "('Epoch', 252, 'Loss', 1.1393130207488293, 'Training Accuracy', 100.0)\n",
      "('Epoch', 253, 'Loss', 1.1359409452356957, 'Training Accuracy', 100.0)\n",
      "('Epoch', 254, 'Loss', 1.1325895056850723, 'Training Accuracy', 100.0)\n",
      "('Epoch', 255, 'Loss', 1.129258509841367, 'Training Accuracy', 100.0)\n",
      "('Epoch', 256, 'Loss', 1.1259477678890188, 'Training Accuracy', 100.0)\n",
      "('Epoch', 257, 'Loss', 1.1226570924123935, 'Training Accuracy', 100.0)\n",
      "('Epoch', 258, 'Loss', 1.1193862983565088, 'Training Accuracy', 100.0)\n",
      "('Epoch', 259, 'Loss', 1.1161352029885685, 'Training Accuracy', 100.0)\n",
      "('Epoch', 260, 'Loss', 1.1129036258602918, 'Training Accuracy', 100.0)\n",
      "('Epoch', 261, 'Loss', 1.1096913887710078, 'Training Accuracy', 100.0)\n",
      "('Epoch', 262, 'Loss', 1.106498315731506, 'Training Accuracy', 100.0)\n",
      "('Epoch', 263, 'Loss', 1.103324232928613, 'Training Accuracy', 100.0)\n",
      "('Epoch', 264, 'Loss', 1.100168968690479, 'Training Accuracy', 100.0)\n",
      "('Epoch', 265, 'Loss', 1.0970323534525803, 'Training Accuracy', 100.0)\n",
      "('Epoch', 266, 'Loss', 1.0939142197243701, 'Training Accuracy', 100.0)\n",
      "('Epoch', 267, 'Loss', 1.0908144020566131, 'Training Accuracy', 100.0)\n",
      "('Epoch', 268, 'Loss', 1.0877327370093628, 'Training Accuracy', 100.0)\n",
      "('Epoch', 269, 'Loss', 1.0846690631205567, 'Training Accuracy', 100.0)\n",
      "('Epoch', 270, 'Loss', 1.081623220875245, 'Training Accuracy', 100.0)\n",
      "('Epoch', 271, 'Loss', 1.0785950526753918, 'Training Accuracy', 100.0)\n",
      "('Epoch', 272, 'Loss', 1.0755844028102952, 'Training Accuracy', 100.0)\n",
      "('Epoch', 273, 'Loss', 1.0725911174275509, 'Training Accuracy', 100.0)\n",
      "('Epoch', 274, 'Loss', 1.0696150445045953, 'Training Accuracy', 100.0)\n",
      "('Epoch', 275, 'Loss', 1.0666560338207636, 'Training Accuracy', 100.0)\n",
      "('Epoch', 276, 'Loss', 1.0637139369299227, 'Training Accuracy', 100.0)\n",
      "('Epoch', 277, 'Loss', 1.0607886071335753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 278, 'Loss', 1.0578798994545187, 'Training Accuracy', 100.0)\n",
      "('Epoch', 279, 'Loss', 1.0549876706109618, 'Training Accuracy', 100.0)\n",
      "('Epoch', 280, 'Loss', 1.0521117789911361, 'Training Accuracy', 100.0)\n",
      "('Epoch', 281, 'Loss', 1.0492520846284032, 'Training Accuracy', 100.0)\n",
      "('Epoch', 282, 'Loss', 1.0464084491767864, 'Training Accuracy', 100.0)\n",
      "('Epoch', 283, 'Loss', 1.0435807358869893, 'Training Accuracy', 100.0)\n",
      "('Epoch', 284, 'Loss', 1.0407688095828243, 'Training Accuracy', 100.0)\n",
      "('Epoch', 285, 'Loss', 1.0379725366381038, 'Training Accuracy', 100.0)\n",
      "('Epoch', 286, 'Loss', 1.0351917849539263, 'Training Accuracy', 100.0)\n",
      "('Epoch', 287, 'Loss', 1.0324264239363978, 'Training Accuracy', 100.0)\n",
      "('Epoch', 288, 'Loss', 1.029676324474738, 'Training Accuracy', 100.0)\n",
      "('Epoch', 289, 'Loss', 1.0269413589197915, 'Training Accuracy', 100.0)\n",
      "('Epoch', 290, 'Loss', 1.0242214010629218, 'Training Accuracy', 100.0)\n",
      "('Epoch', 291, 'Loss', 1.0215163261152853, 'Training Accuracy', 100.0)\n",
      "('Epoch', 292, 'Loss', 1.0188260106874758, 'Training Accuracy', 100.0)\n",
      "('Epoch', 293, 'Loss', 1.0161503327695147, 'Training Accuracy', 100.0)\n",
      "('Epoch', 294, 'Loss', 1.0134891717112218, 'Training Accuracy', 100.0)\n",
      "('Epoch', 295, 'Loss', 1.0108424082029066, 'Training Accuracy', 100.0)\n",
      "('Epoch', 296, 'Loss', 1.0082099242564162, 'Training Accuracy', 100.0)\n",
      "('Epoch', 297, 'Loss', 1.0055916031864991, 'Training Accuracy', 100.0)\n",
      "('Epoch', 298, 'Loss', 1.0029873295925062, 'Training Accuracy', 100.0)\n",
      "('Epoch', 299, 'Loss', 1.0003969893404048, 'Training Accuracy', 100.0)\n",
      "('Epoch', 300, 'Loss', 0.99782046954509807, 'Training Accuracy', 100.0)\n",
      "('Epoch', 301, 'Loss', 0.99525765855305437, 'Training Accuracy', 100.0)\n",
      "('Epoch', 302, 'Loss', 0.99270844592523599, 'Training Accuracy', 100.0)\n",
      "('Epoch', 303, 'Loss', 0.99017272242030796, 'Training Accuracy', 100.0)\n",
      "('Epoch', 304, 'Loss', 0.98765037997815552, 'Training Accuracy', 100.0)\n",
      "('Epoch', 305, 'Loss', 0.98514131170365349, 'Training Accuracy', 100.0)\n",
      "('Epoch', 306, 'Loss', 0.98264541185072884, 'Training Accuracy', 100.0)\n",
      "('Epoch', 307, 'Loss', 0.98016257580668631, 'Training Accuracy', 100.0)\n",
      "('Epoch', 308, 'Loss', 0.97769270007679121, 'Training Accuracy', 100.0)\n",
      "('Epoch', 309, 'Loss', 0.97523568226912405, 'Training Accuracy', 100.0)\n",
      "('Epoch', 310, 'Loss', 0.97279142107967098, 'Training Accuracy', 100.0)\n",
      "('Epoch', 311, 'Loss', 0.97035981627767054, 'Training Accuracy', 100.0)\n",
      "('Epoch', 312, 'Loss', 0.96794076869120682, 'Training Accuracy', 100.0)\n",
      "('Epoch', 313, 'Loss', 0.96553418019302595, 'Training Accuracy', 100.0)\n",
      "('Epoch', 314, 'Loss', 0.96313995368660255, 'Training Accuracy', 100.0)\n",
      "('Epoch', 315, 'Loss', 0.96075799309242182, 'Training Accuracy', 100.0)\n",
      "('Epoch', 316, 'Loss', 0.95838820333448937, 'Training Accuracy', 100.0)\n",
      "('Epoch', 317, 'Loss', 0.95603049032706378, 'Training Accuracy', 100.0)\n",
      "('Epoch', 318, 'Loss', 0.95368476096159904, 'Training Accuracy', 100.0)\n",
      "('Epoch', 319, 'Loss', 0.95135092309389713, 'Training Accuracy', 100.0)\n",
      "('Epoch', 320, 'Loss', 0.94902888553147957, 'Training Accuracy', 100.0)\n",
      "('Epoch', 321, 'Loss', 0.94671855802114158, 'Training Accuracy', 100.0)\n",
      "('Epoch', 322, 'Loss', 0.94441985123671701, 'Training Accuracy', 100.0)\n",
      "('Epoch', 323, 'Loss', 0.94213267676704693, 'Training Accuracy', 100.0)\n",
      "('Epoch', 324, 'Loss', 0.93985694710411927, 'Training Accuracy', 100.0)\n",
      "('Epoch', 325, 'Loss', 0.93759257563141341, 'Training Accuracy', 100.0)\n",
      "('Epoch', 326, 'Loss', 0.93533947661241301, 'Training Accuracy', 100.0)\n",
      "('Epoch', 327, 'Loss', 0.93309756517931886, 'Training Accuracy', 100.0)\n",
      "('Epoch', 328, 'Loss', 0.93086675732192747, 'Training Accuracy', 100.0)\n",
      "('Epoch', 329, 'Loss', 0.92864696987666906, 'Training Accuracy', 100.0)\n",
      "('Epoch', 330, 'Loss', 0.92643812051585672, 'Training Accuracy', 100.0)\n",
      "('Epoch', 331, 'Loss', 0.92424012773705666, 'Training Accuracy', 100.0)\n",
      "('Epoch', 332, 'Loss', 0.92205291085265417, 'Training Accuracy', 100.0)\n",
      "('Epoch', 333, 'Loss', 0.91987638997957022, 'Training Accuracy', 100.0)\n",
      "('Epoch', 334, 'Loss', 0.91771048602913186, 'Training Accuracy', 100.0)\n",
      "('Epoch', 335, 'Loss', 0.91555512069710965, 'Training Accuracy', 100.0)\n",
      "('Epoch', 336, 'Loss', 0.91341021645389608, 'Training Accuracy', 100.0)\n",
      "('Epoch', 337, 'Loss', 0.91127569653483986, 'Training Accuracy', 100.0)\n",
      "('Epoch', 338, 'Loss', 0.90915148493072595, 'Training Accuracy', 100.0)\n",
      "('Epoch', 339, 'Loss', 0.9070375063784033, 'Training Accuracy', 100.0)\n",
      "('Epoch', 340, 'Loss', 0.90493368635154137, 'Training Accuracy', 100.0)\n",
      "('Epoch', 341, 'Loss', 0.90283995105154657, 'Training Accuracy', 100.0)\n",
      "('Epoch', 342, 'Loss', 0.90075622739859729, 'Training Accuracy', 100.0)\n",
      "('Epoch', 343, 'Loss', 0.89868244302282441, 'Training Accuracy', 100.0)\n",
      "('Epoch', 344, 'Loss', 0.89661852625561167, 'Training Accuracy', 100.0)\n",
      "('Epoch', 345, 'Loss', 0.89456440612104582, 'Training Accuracy', 100.0)\n",
      "('Epoch', 346, 'Loss', 0.89252001232746847, 'Training Accuracy', 100.0)\n",
      "('Epoch', 347, 'Loss', 0.89048527525916876, 'Training Accuracy', 100.0)\n",
      "('Epoch', 348, 'Loss', 0.88846012596820279, 'Training Accuracy', 100.0)\n",
      "('Epoch', 349, 'Loss', 0.88644449616632026, 'Training Accuracy', 100.0)\n",
      "('Epoch', 350, 'Loss', 0.88443831821700991, 'Training Accuracy', 100.0)\n",
      "('Epoch', 351, 'Loss', 0.88244152512768415, 'Training Accuracy', 100.0)\n",
      "('Epoch', 352, 'Loss', 0.88045405054194459, 'Training Accuracy', 100.0)\n",
      "('Epoch', 353, 'Loss', 0.87847582873198227, 'Training Accuracy', 100.0)\n",
      "('Epoch', 354, 'Loss', 0.87650679459108438, 'Training Accuracy', 100.0)\n",
      "('Epoch', 355, 'Loss', 0.874546883626246, 'Training Accuracy', 100.0)\n",
      "('Epoch', 356, 'Loss', 0.87259603195088753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 357, 'Loss', 0.87065417627768715, 'Training Accuracy', 100.0)\n",
      "('Epoch', 358, 'Loss', 0.86872125391149291, 'Training Accuracy', 100.0)\n",
      "('Epoch', 359, 'Loss', 0.86679720274237571, 'Training Accuracy', 100.0)\n",
      "('Epoch', 360, 'Loss', 0.86488196123873273, 'Training Accuracy', 100.0)\n",
      "('Epoch', 361, 'Loss', 0.8629754684405303, 'Training Accuracy', 100.0)\n",
      "('Epoch', 362, 'Loss', 0.86107766395262098, 'Training Accuracy', 100.0)\n",
      "('Epoch', 363, 'Loss', 0.85918848793816238, 'Training Accuracy', 100.0)\n",
      "('Epoch', 364, 'Loss', 0.8573078811121233, 'Training Accuracy', 100.0)\n",
      "('Epoch', 365, 'Loss', 0.85543578473489412, 'Training Accuracy', 100.0)\n",
      "('Epoch', 366, 'Loss', 0.85357214060596753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 367, 'Loss', 0.85171689105772597, 'Training Accuracy', 100.0)\n",
      "('Epoch', 368, 'Loss', 0.84986997894930838, 'Training Accuracy', 100.0)\n",
      "('Epoch', 369, 'Loss', 0.84803134766055732, 'Training Accuracy', 100.0)\n",
      "('Epoch', 370, 'Loss', 0.84620094108605837, 'Training Accuracy', 100.0)\n",
      "('Epoch', 371, 'Loss', 0.84437870362926981, 'Training Accuracy', 100.0)\n",
      "('Epoch', 372, 'Loss', 0.84256458019669933, 'Training Accuracy', 100.0)\n",
      "('Epoch', 373, 'Loss', 0.84075851619220965, 'Training Accuracy', 100.0)\n",
      "('Epoch', 374, 'Loss', 0.83896045751136161, 'Training Accuracy', 100.0)\n",
      "('Epoch', 375, 'Loss', 0.83717035053585864, 'Training Accuracy', 100.0)\n",
      "('Epoch', 376, 'Loss', 0.83538814212805657, 'Training Accuracy', 100.0)\n",
      "('Epoch', 377, 'Loss', 0.83361377962555272, 'Training Accuracy', 100.0)\n",
      "('Epoch', 378, 'Loss', 0.83184721083584923, 'Training Accuracy', 100.0)\n",
      "('Epoch', 379, 'Loss', 0.83008838403108476, 'Training Accuracy', 100.0)\n",
      "('Epoch', 380, 'Loss', 0.82833724794283536, 'Training Accuracy', 100.0)\n",
      "('Epoch', 381, 'Loss', 0.82659375175700311, 'Training Accuracy', 100.0)\n",
      "('Epoch', 382, 'Loss', 0.82485784510874938, 'Training Accuracy', 100.0)\n",
      "('Epoch', 383, 'Loss', 0.82312947807751813, 'Training Accuracy', 100.0)\n",
      "('Epoch', 384, 'Loss', 0.8214086011820948, 'Training Accuracy', 100.0)\n",
      "('Epoch', 385, 'Loss', 0.81969516537578202, 'Training Accuracy', 100.0)\n",
      "('Epoch', 386, 'Loss', 0.8179891220415888, 'Training Accuracy', 100.0)\n",
      "('Epoch', 387, 'Loss', 0.81629042298750976, 'Training Accuracy', 100.0)\n",
      "('Epoch', 388, 'Loss', 0.81459902044186627, 'Training Accuracy', 100.0)\n",
      "('Epoch', 389, 'Loss', 0.81291486704870108, 'Training Accuracy', 100.0)\n",
      "('Epoch', 390, 'Loss', 0.81123791586324012, 'Training Accuracy', 100.0)\n",
      "('Epoch', 391, 'Loss', 0.80956812034741044, 'Training Accuracy', 100.0)\n",
      "('Epoch', 392, 'Loss', 0.8079054343654235, 'Training Accuracy', 100.0)\n",
      "('Epoch', 393, 'Loss', 0.80624981217940517, 'Training Accuracy', 100.0)\n",
      "('Epoch', 394, 'Loss', 0.80460120844509708, 'Training Accuracy', 100.0)\n",
      "('Epoch', 395, 'Loss', 0.80295957820759811, 'Training Accuracy', 100.0)\n",
      "('Epoch', 396, 'Loss', 0.8013248768971778, 'Training Accuracy', 100.0)\n",
      "('Epoch', 397, 'Loss', 0.79969706032513443, 'Training Accuracy', 100.0)\n",
      "('Epoch', 398, 'Loss', 0.79807608467970503, 'Training Accuracy', 100.0)\n",
      "('Epoch', 399, 'Loss', 0.79646190652203352, 'Training Accuracy', 100.0)\n",
      "('Epoch', 400, 'Loss', 0.79485448278220039, 'Training Accuracy', 100.0)\n",
      "('Epoch', 401, 'Loss', 0.79325377075527259, 'Training Accuracy', 100.0)\n",
      "('Epoch', 402, 'Loss', 0.7916597280974399, 'Training Accuracy', 100.0)\n",
      "('Epoch', 403, 'Loss', 0.79007231282217982, 'Training Accuracy', 100.0)\n",
      "('Epoch', 404, 'Loss', 0.78849148329647678, 'Training Accuracy', 100.0)\n",
      "('Epoch', 405, 'Loss', 0.7869171982370935, 'Training Accuracy', 100.0)\n",
      "('Epoch', 406, 'Loss', 0.7853494167068803, 'Training Accuracy', 100.0)\n",
      "('Epoch', 407, 'Loss', 0.78378809811114247, 'Training Accuracy', 100.0)\n",
      "('Epoch', 408, 'Loss', 0.78223320219404302, 'Training Accuracy', 100.0)\n",
      "('Epoch', 409, 'Loss', 0.78068468903506283, 'Training Accuracy', 100.0)\n",
      "('Epoch', 410, 'Loss', 0.77914251904549492, 'Training Accuracy', 100.0)\n",
      "('Epoch', 411, 'Loss', 0.77760665296498654, 'Training Accuracy', 100.0)\n",
      "('Epoch', 412, 'Loss', 0.77607705185812503, 'Training Accuracy', 100.0)\n",
      "('Epoch', 413, 'Loss', 0.77455367711107059, 'Training Accuracy', 100.0)\n",
      "('Epoch', 414, 'Loss', 0.77303649042822331, 'Training Accuracy', 100.0)\n",
      "('Epoch', 415, 'Loss', 0.77152545382893212, 'Training Accuracy', 100.0)\n",
      "('Epoch', 416, 'Loss', 0.77002052964426237, 'Training Accuracy', 100.0)\n",
      "('Epoch', 417, 'Loss', 0.76852168051377057, 'Training Accuracy', 100.0)\n",
      "('Epoch', 418, 'Loss', 0.76702886938234749, 'Training Accuracy', 100.0)\n",
      "('Epoch', 419, 'Loss', 0.76554205949709409, 'Training Accuracy', 100.0)\n",
      "('Epoch', 420, 'Loss', 0.7640612144042267, 'Training Accuracy', 100.0)\n",
      "('Epoch', 421, 'Loss', 0.76258629794602972, 'Training Accuracy', 100.0)\n",
      "('Epoch', 422, 'Loss', 0.76111727425784448, 'Training Accuracy', 100.0)\n",
      "('Epoch', 423, 'Loss', 0.7596541077650778, 'Training Accuracy', 100.0)\n",
      "('Epoch', 424, 'Loss', 0.75819676318028706, 'Training Accuracy', 100.0)\n",
      "('Epoch', 425, 'Loss', 0.75674520550025792, 'Training Accuracy', 100.0)\n",
      "('Epoch', 426, 'Loss', 0.75529940000313656, 'Training Accuracy', 100.0)\n",
      "('Epoch', 427, 'Loss', 0.75385931224560765, 'Training Accuracy', 100.0)\n",
      "('Epoch', 428, 'Loss', 0.75242490806008433, 'Training Accuracy', 100.0)\n",
      "('Epoch', 429, 'Loss', 0.75099615355194527, 'Training Accuracy', 100.0)\n",
      "('Epoch', 430, 'Loss', 0.74957301509680485, 'Training Accuracy', 100.0)\n",
      "('Epoch', 431, 'Loss', 0.74815545933781047, 'Training Accuracy', 100.0)\n",
      "('Epoch', 432, 'Loss', 0.7467434531829823, 'Training Accuracy', 100.0)\n",
      "('Epoch', 433, 'Loss', 0.74533696380256853, 'Training Accuracy', 100.0)\n",
      "('Epoch', 434, 'Loss', 0.74393595862645423, 'Training Accuracy', 100.0)\n",
      "('Epoch', 435, 'Loss', 0.74254040534157784, 'Training Accuracy', 100.0)\n",
      "('Epoch', 436, 'Loss', 0.74115027188939964, 'Training Accuracy', 100.0)\n",
      "('Epoch', 437, 'Loss', 0.73976552646338212, 'Training Accuracy', 100.0)\n",
      "('Epoch', 438, 'Loss', 0.73838613750651616, 'Training Accuracy', 100.0)\n",
      "('Epoch', 439, 'Loss', 0.73701207370886657, 'Training Accuracy', 100.0)\n",
      "('Epoch', 440, 'Loss', 0.7356433040051451, 'Training Accuracy', 100.0)\n",
      "('Epoch', 441, 'Loss', 0.73427979757232154, 'Training Accuracy', 100.0)\n",
      "('Epoch', 442, 'Loss', 0.7329215238272565, 'Training Accuracy', 100.0)\n",
      "('Epoch', 443, 'Loss', 0.73156845242436319, 'Training Accuracy', 100.0)\n",
      "('Epoch', 444, 'Loss', 0.73022055325329316, 'Training Accuracy', 100.0)\n",
      "('Epoch', 445, 'Loss', 0.72887779643665895, 'Training Accuracy', 100.0)\n",
      "('Epoch', 446, 'Loss', 0.72754015232777336, 'Training Accuracy', 100.0)\n",
      "('Epoch', 447, 'Loss', 0.72620759150841652, 'Training Accuracy', 100.0)\n",
      "('Epoch', 448, 'Loss', 0.72488008478663479, 'Training Accuracy', 100.0)\n",
      "('Epoch', 449, 'Loss', 0.72355760319455964, 'Training Accuracy', 100.0)\n",
      "('Epoch', 450, 'Loss', 0.72224011798625609, 'Training Accuracy', 100.0)\n",
      "('Epoch', 451, 'Loss', 0.7209276006355898, 'Training Accuracy', 100.0)\n",
      "('Epoch', 452, 'Loss', 0.71962002283412474, 'Training Accuracy', 100.0)\n",
      "('Epoch', 453, 'Loss', 0.71831735648904826, 'Training Accuracy', 100.0)\n",
      "('Epoch', 454, 'Loss', 0.71701957372110026, 'Training Accuracy', 100.0)\n",
      "('Epoch', 455, 'Loss', 0.71572664686255916, 'Training Accuracy', 100.0)\n",
      "('Epoch', 456, 'Loss', 0.7144385484552167, 'Training Accuracy', 100.0)\n",
      "('Epoch', 457, 'Loss', 0.71315525124839996, 'Training Accuracy', 100.0)\n",
      "('Epoch', 458, 'Loss', 0.71187672819700643, 'Training Accuracy', 100.0)\n",
      "('Epoch', 459, 'Loss', 0.71060295245956007, 'Training Accuracy', 100.0)\n",
      "('Epoch', 460, 'Loss', 0.70933389739629427, 'Training Accuracy', 100.0)\n",
      "('Epoch', 461, 'Loss', 0.70806953656725469, 'Training Accuracy', 100.0)\n",
      "('Epoch', 462, 'Loss', 0.70680984373041644, 'Training Accuracy', 100.0)\n",
      "('Epoch', 463, 'Loss', 0.70555479283984024, 'Training Accuracy', 100.0)\n",
      "('Epoch', 464, 'Loss', 0.70430435804383051, 'Training Accuracy', 100.0)\n",
      "('Epoch', 465, 'Loss', 0.70305851368312133, 'Training Accuracy', 100.0)\n",
      "('Epoch', 466, 'Loss', 0.70181723428908704, 'Training Accuracy', 100.0)\n",
      "('Epoch', 467, 'Loss', 0.70058049458197225, 'Training Accuracy', 100.0)\n",
      "('Epoch', 468, 'Loss', 0.69934826946912876, 'Training Accuracy', 100.0)\n",
      "('Epoch', 469, 'Loss', 0.69812053404329089, 'Training Accuracy', 100.0)\n",
      "('Epoch', 470, 'Loss', 0.69689726358085435, 'Training Accuracy', 100.0)\n",
      "('Epoch', 471, 'Loss', 0.69567843354018877, 'Training Accuracy', 100.0)\n",
      "('Epoch', 472, 'Loss', 0.69446401955995551, 'Training Accuracy', 100.0)\n",
      "('Epoch', 473, 'Loss', 0.6932539974574492, 'Training Accuracy', 100.0)\n",
      "('Epoch', 474, 'Loss', 0.69204834322696251, 'Training Accuracy', 100.0)\n",
      "('Epoch', 475, 'Loss', 0.69084703303816075, 'Training Accuracy', 100.0)\n",
      "('Epoch', 476, 'Loss', 0.68965004323448376, 'Training Accuracy', 100.0)\n",
      "('Epoch', 477, 'Loss', 0.68845735033155109, 'Training Accuracy', 100.0)\n",
      "('Epoch', 478, 'Loss', 0.68726893101560216, 'Training Accuracy', 100.0)\n",
      "('Epoch', 479, 'Loss', 0.68608476214193714, 'Training Accuracy', 100.0)\n",
      "('Epoch', 480, 'Loss', 0.68490482073339187, 'Training Accuracy', 100.0)\n",
      "('Epoch', 481, 'Loss', 0.68372908397881271, 'Training Accuracy', 100.0)\n",
      "('Epoch', 482, 'Loss', 0.68255752923155455, 'Training Accuracy', 100.0)\n",
      "('Epoch', 483, 'Loss', 0.6813901340080043, 'Training Accuracy', 100.0)\n",
      "('Epoch', 484, 'Loss', 0.68022687598610132, 'Training Accuracy', 100.0)\n",
      "('Epoch', 485, 'Loss', 0.67906773300389212, 'Training Accuracy', 100.0)\n",
      "('Epoch', 486, 'Loss', 0.67791268305809393, 'Training Accuracy', 100.0)\n",
      "('Epoch', 487, 'Loss', 0.67676170430266602, 'Training Accuracy', 100.0)\n",
      "('Epoch', 488, 'Loss', 0.67561477504741085, 'Training Accuracy', 100.0)\n",
      "('Epoch', 489, 'Loss', 0.6744718737565768, 'Training Accuracy', 100.0)\n",
      "('Epoch', 490, 'Loss', 0.67333297904748579, 'Training Accuracy', 100.0)\n",
      "('Epoch', 491, 'Loss', 0.67219806968917217, 'Training Accuracy', 100.0)\n",
      "('Epoch', 492, 'Loss', 0.67106712460102924, 'Training Accuracy', 100.0)\n",
      "('Epoch', 493, 'Loss', 0.66994012285148674, 'Training Accuracy', 100.0)\n",
      "('Epoch', 494, 'Loss', 0.66881704365668637, 'Training Accuracy', 100.0)\n",
      "('Epoch', 495, 'Loss', 0.6676978663791735, 'Training Accuracy', 100.0)\n",
      "('Epoch', 496, 'Loss', 0.66658257052661529, 'Training Accuracy', 100.0)\n",
      "('Epoch', 497, 'Loss', 0.66547113575052197, 'Training Accuracy', 100.0)\n",
      "('Epoch', 498, 'Loss', 0.66436354184497437, 'Training Accuracy', 100.0)\n",
      "('Epoch', 499, 'Loss', 0.66325976874538417, 'Training Accuracy', 100.0)\n",
      "('Epoch', 500, 'Loss', 0.66215979652725732, 'Training Accuracy', 100.0)\n",
      "('Epoch', 501, 'Loss', 0.66106360540495679, 'Training Accuracy', 100.0)\n",
      "('Epoch', 502, 'Loss', 0.65997117573050978, 'Training Accuracy', 100.0)\n",
      "('Epoch', 503, 'Loss', 0.65888248799239579, 'Training Accuracy', 100.0)\n",
      "('Epoch', 504, 'Loss', 0.65779752281436754, 'Training Accuracy', 100.0)\n",
      "('Epoch', 505, 'Loss', 0.65671626095427227, 'Training Accuracy', 100.0)\n",
      "('Epoch', 506, 'Loss', 0.65563868330289954, 'Training Accuracy', 100.0)\n",
      "('Epoch', 507, 'Loss', 0.65456477088281795, 'Training Accuracy', 100.0)\n",
      "('Epoch', 508, 'Loss', 0.65349450484725391, 'Training Accuracy', 100.0)\n",
      "('Epoch', 509, 'Loss', 0.65242786647895301, 'Training Accuracy', 100.0)\n",
      "('Epoch', 510, 'Loss', 0.65136483718907334, 'Training Accuracy', 100.0)\n",
      "('Epoch', 511, 'Loss', 0.65030539851608526, 'Training Accuracy', 100.0)\n",
      "('Epoch', 512, 'Loss', 0.64924953212466863, 'Training Accuracy', 100.0)\n",
      "('Epoch', 513, 'Loss', 0.64819721980464839, 'Training Accuracy', 100.0)\n",
      "('Epoch', 514, 'Loss', 0.64714844346991507, 'Training Accuracy', 100.0)\n",
      "('Epoch', 515, 'Loss', 0.64610318515737242, 'Training Accuracy', 100.0)\n",
      "('Epoch', 516, 'Loss', 0.64506142702588731, 'Training Accuracy', 100.0)\n",
      "('Epoch', 517, 'Loss', 0.64402315135525767, 'Training Accuracy', 100.0)\n",
      "('Epoch', 518, 'Loss', 0.64298834054518439, 'Training Accuracy', 100.0)\n",
      "('Epoch', 519, 'Loss', 0.6419569771142517, 'Training Accuracy', 100.0)\n",
      "('Epoch', 520, 'Loss', 0.64092904369893389, 'Training Accuracy', 100.0)\n",
      "('Epoch', 521, 'Loss', 0.63990452305259071, 'Training Accuracy', 100.0)\n",
      "('Epoch', 522, 'Loss', 0.63888339804448335, 'Training Accuracy', 100.0)\n",
      "('Epoch', 523, 'Loss', 0.6378656516588086, 'Training Accuracy', 100.0)\n",
      "('Epoch', 524, 'Loss', 0.63685126699372319, 'Training Accuracy', 100.0)\n",
      "('Epoch', 525, 'Loss', 0.63584022726039746, 'Training Accuracy', 100.0)\n",
      "('Epoch', 526, 'Loss', 0.63483251578206212, 'Training Accuracy', 100.0)\n",
      "('Epoch', 527, 'Loss', 0.63382811599308242, 'Training Accuracy', 100.0)\n",
      "('Epoch', 528, 'Loss', 0.63282701143802178, 'Training Accuracy', 100.0)\n",
      "('Epoch', 529, 'Loss', 0.63182918577072911, 'Training Accuracy', 100.0)\n",
      "('Epoch', 530, 'Loss', 0.63083462275343172, 'Training Accuracy', 100.0)\n",
      "('Epoch', 531, 'Loss', 0.6298433062558314, 'Training Accuracy', 100.0)\n",
      "('Epoch', 532, 'Loss', 0.62885522025422358, 'Training Accuracy', 100.0)\n",
      "('Epoch', 533, 'Loss', 0.62787034883060744, 'Training Accuracy', 100.0)\n",
      "('Epoch', 534, 'Loss', 0.62688867617181754, 'Training Accuracy', 100.0)\n",
      "('Epoch', 535, 'Loss', 0.62591018656865671, 'Training Accuracy', 100.0)\n",
      "('Epoch', 536, 'Loss', 0.6249348644150452, 'Training Accuracy', 100.0)\n",
      "('Epoch', 537, 'Loss', 0.62396269420716954, 'Training Accuracy', 100.0)\n",
      "('Epoch', 538, 'Loss', 0.62299366054265326, 'Training Accuracy', 100.0)\n",
      "('Epoch', 539, 'Loss', 0.62202774811971084, 'Training Accuracy', 100.0)\n",
      "('Epoch', 540, 'Loss', 0.62106494173634086, 'Training Accuracy', 100.0)\n",
      "('Epoch', 541, 'Loss', 0.6201052262895056, 'Training Accuracy', 100.0)\n",
      "('Epoch', 542, 'Loss', 0.6191485867743195, 'Training Accuracy', 100.0)\n",
      "('Epoch', 543, 'Loss', 0.61819500828326579, 'Training Accuracy', 100.0)\n",
      "('Epoch', 544, 'Loss', 0.61724447600539045, 'Training Accuracy', 100.0)\n",
      "('Epoch', 545, 'Loss', 0.61629697522552762, 'Training Accuracy', 100.0)\n",
      "('Epoch', 546, 'Loss', 0.61535249132352976, 'Training Accuracy', 100.0)\n",
      "('Epoch', 547, 'Loss', 0.61441100977348351, 'Training Accuracy', 100.0)\n",
      "('Epoch', 548, 'Loss', 0.61347251614296772, 'Training Accuracy', 100.0)\n",
      "('Epoch', 549, 'Loss', 0.61253699609228818, 'Training Accuracy', 100.0)\n",
      "('Epoch', 550, 'Loss', 0.61160443537374354, 'Training Accuracy', 100.0)\n",
      "('Epoch', 551, 'Loss', 0.61067481983087268, 'Training Accuracy', 100.0)\n",
      "('Epoch', 552, 'Loss', 0.60974813539773853, 'Training Accuracy', 100.0)\n",
      "('Epoch', 553, 'Loss', 0.60882436809819507, 'Training Accuracy', 100.0)\n",
      "('Epoch', 554, 'Loss', 0.60790350404516946, 'Training Accuracy', 100.0)\n",
      "('Epoch', 555, 'Loss', 0.60698552943995554, 'Training Accuracy', 100.0)\n",
      "('Epoch', 556, 'Loss', 0.60607043057151377, 'Training Accuracy', 100.0)\n",
      "('Epoch', 557, 'Loss', 0.60515819381576585, 'Training Accuracy', 100.0)\n",
      "('Epoch', 558, 'Loss', 0.60424880563491201, 'Training Accuracy', 100.0)\n",
      "('Epoch', 559, 'Loss', 0.60334225257673968, 'Training Accuracy', 100.0)\n",
      "('Epoch', 560, 'Loss', 0.60243852127395747, 'Training Accuracy', 100.0)\n",
      "('Epoch', 561, 'Loss', 0.60153759844351617, 'Training Accuracy', 100.0)\n",
      "('Epoch', 562, 'Loss', 0.60063947088594649, 'Training Accuracy', 100.0)\n",
      "('Epoch', 563, 'Loss', 0.59974412548470135, 'Training Accuracy', 100.0)\n",
      "('Epoch', 564, 'Loss', 0.59885154920550543, 'Training Accuracy', 100.0)\n",
      "('Epoch', 565, 'Loss', 0.59796172909570866, 'Training Accuracy', 100.0)\n",
      "('Epoch', 566, 'Loss', 0.59707465228364609, 'Training Accuracy', 100.0)\n",
      "('Epoch', 567, 'Loss', 0.59619030597800415, 'Training Accuracy', 100.0)\n",
      "('Epoch', 568, 'Loss', 0.59530867746719773, 'Training Accuracy', 100.0)\n",
      "('Epoch', 569, 'Loss', 0.59442975411874022, 'Training Accuracy', 100.0)\n",
      "('Epoch', 570, 'Loss', 0.59355352337863521, 'Training Accuracy', 100.0)\n",
      "('Epoch', 571, 'Loss', 0.5926799727707639, 'Training Accuracy', 100.0)\n",
      "('Epoch', 572, 'Loss', 0.59180908989628112, 'Training Accuracy', 100.0)\n",
      "('Epoch', 573, 'Loss', 0.59094086243301225, 'Training Accuracy', 100.0)\n",
      "('Epoch', 574, 'Loss', 0.59007527813486627, 'Training Accuracy', 100.0)\n",
      "('Epoch', 575, 'Loss', 0.58921232483124875, 'Training Accuracy', 100.0)\n",
      "('Epoch', 576, 'Loss', 0.58835199042646846, 'Training Accuracy', 100.0)\n",
      "('Epoch', 577, 'Loss', 0.58749426289917928, 'Training Accuracy', 100.0)\n",
      "('Epoch', 578, 'Loss', 0.58663913030178927, 'Training Accuracy', 100.0)\n",
      "('Epoch', 579, 'Loss', 0.58578658075990475, 'Training Accuracy', 100.0)\n",
      "('Epoch', 580, 'Loss', 0.58493660247177182, 'Training Accuracy', 100.0)\n",
      "('Epoch', 581, 'Loss', 0.58408918370771346, 'Training Accuracy', 100.0)\n",
      "('Epoch', 582, 'Loss', 0.58324431280958433, 'Training Accuracy', 100.0)\n",
      "('Epoch', 583, 'Loss', 0.58240197819021688, 'Training Accuracy', 100.0)\n",
      "('Epoch', 584, 'Loss', 0.58156216833289376, 'Training Accuracy', 100.0)\n",
      "('Epoch', 585, 'Loss', 0.58072487179080234, 'Training Accuracy', 100.0)\n",
      "('Epoch', 586, 'Loss', 0.57989007718650232, 'Training Accuracy', 100.0)\n",
      "('Epoch', 587, 'Loss', 0.57905777321141128, 'Training Accuracy', 100.0)\n",
      "('Epoch', 588, 'Loss', 0.5782279486252665, 'Training Accuracy', 100.0)\n",
      "('Epoch', 589, 'Loss', 0.57740059225562856, 'Training Accuracy', 100.0)\n",
      "('Epoch', 590, 'Loss', 0.57657569299734901, 'Training Accuracy', 100.0)\n",
      "('Epoch', 591, 'Loss', 0.57575323981207982, 'Training Accuracy', 100.0)\n",
      "('Epoch', 592, 'Loss', 0.57493322172776151, 'Training Accuracy', 100.0)\n",
      "('Epoch', 593, 'Loss', 0.57411562783812708, 'Training Accuracy', 100.0)\n",
      "('Epoch', 594, 'Loss', 0.57330044730220919, 'Training Accuracy', 100.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 595, 'Loss', 0.57248766934385276, 'Training Accuracy', 100.0)\n",
      "('Epoch', 596, 'Loss', 0.57167728325122813, 'Training Accuracy', 100.0)\n",
      "('Epoch', 597, 'Loss', 0.57086927837635015, 'Training Accuracy', 100.0)\n",
      "('Epoch', 598, 'Loss', 0.57006364413460064, 'Training Accuracy', 100.0)\n",
      "('Epoch', 599, 'Loss', 0.56926037000426355, 'Training Accuracy', 100.0)\n",
      "('Epoch', 600, 'Loss', 0.56845944552605376, 'Training Accuracy', 100.0)\n",
      "('Epoch', 601, 'Loss', 0.56766086030264351, 'Training Accuracy', 100.0)\n",
      "('Epoch', 602, 'Loss', 0.56686460399822225, 'Training Accuracy', 100.0)\n",
      "('Epoch', 603, 'Loss', 0.56607066633802983, 'Training Accuracy', 100.0)\n",
      "('Epoch', 604, 'Loss', 0.56527903710790373, 'Training Accuracy', 100.0)\n",
      "('Epoch', 605, 'Loss', 0.56448970615383476, 'Training Accuracy', 100.0)\n",
      "('Epoch', 606, 'Loss', 0.56370266338152997, 'Training Accuracy', 100.0)\n",
      "('Epoch', 607, 'Loss', 0.56291789875596199, 'Training Accuracy', 100.0)\n",
      "('Epoch', 608, 'Loss', 0.56213540230094727, 'Training Accuracy', 100.0)\n",
      "('Epoch', 609, 'Loss', 0.5613551640986929, 'Training Accuracy', 100.0)\n",
      "('Epoch', 610, 'Loss', 0.56057717428939946, 'Training Accuracy', 100.0)\n",
      "('Epoch', 611, 'Loss', 0.55980142307080905, 'Training Accuracy', 100.0)\n",
      "('Epoch', 612, 'Loss', 0.55902790069779884, 'Training Accuracy', 100.0)\n",
      "('Epoch', 613, 'Loss', 0.55825659748196588, 'Training Accuracy', 100.0)\n",
      "('Epoch', 614, 'Loss', 0.55748750379120515, 'Training Accuracy', 100.0)\n",
      "('Epoch', 615, 'Loss', 0.55672061004930795, 'Training Accuracy', 100.0)\n",
      "('Epoch', 616, 'Loss', 0.55595590673555406, 'Training Accuracy', 100.0)\n",
      "('Epoch', 617, 'Loss', 0.55519338438430688, 'Training Accuracy', 100.0)\n",
      "('Epoch', 618, 'Loss', 0.55443303358461726, 'Training Accuracy', 100.0)\n",
      "('Epoch', 619, 'Loss', 0.55367484497983099, 'Training Accuracy', 100.0)\n",
      "('Epoch', 620, 'Loss', 0.55291880926718817, 'Training Accuracy', 100.0)\n",
      "('Epoch', 621, 'Loss', 0.55216491719744665, 'Training Accuracy', 100.0)\n",
      "('Epoch', 622, 'Loss', 0.55141315957448633, 'Training Accuracy', 100.0)\n",
      "('Epoch', 623, 'Loss', 0.55066352725493373, 'Training Accuracy', 100.0)\n",
      "('Epoch', 624, 'Loss', 0.54991601114778155, 'Training Accuracy', 100.0)\n",
      "('Epoch', 625, 'Loss', 0.54917060221401126, 'Training Accuracy', 100.0)\n",
      "('Epoch', 626, 'Loss', 0.54842729146622693, 'Training Accuracy', 100.0)\n",
      "('Epoch', 627, 'Loss', 0.54768606996827973, 'Training Accuracy', 100.0)\n",
      "('Epoch', 628, 'Loss', 0.5469469288349097, 'Training Accuracy', 100.0)\n",
      "('Epoch', 629, 'Loss', 0.54620985923137544, 'Training Accuracy', 100.0)\n",
      "('Epoch', 630, 'Loss', 0.54547485237309912, 'Training Accuracy', 100.0)\n",
      "('Epoch', 631, 'Loss', 0.54474189952530849, 'Training Accuracy', 100.0)\n",
      "('Epoch', 632, 'Loss', 0.54401099200268888, 'Training Accuracy', 100.0)\n",
      "('Epoch', 633, 'Loss', 0.54328212116902252, 'Training Accuracy', 100.0)\n",
      "('Epoch', 634, 'Loss', 0.54255527843685192, 'Training Accuracy', 100.0)\n",
      "('Epoch', 635, 'Loss', 0.54183045526712903, 'Training Accuracy', 100.0)\n",
      "('Epoch', 636, 'Loss', 0.54110764316887794, 'Training Accuracy', 100.0)\n",
      "('Epoch', 637, 'Loss', 0.54038683369884877, 'Training Accuracy', 100.0)\n",
      "('Epoch', 638, 'Loss', 0.5396680184611925, 'Training Accuracy', 100.0)\n",
      "('Epoch', 639, 'Loss', 0.53895118910712192, 'Training Accuracy', 100.0)\n",
      "('Epoch', 640, 'Loss', 0.53823633733458343, 'Training Accuracy', 100.0)\n",
      "('Epoch', 641, 'Loss', 0.53752345488792341, 'Training Accuracy', 100.0)\n",
      "('Epoch', 642, 'Loss', 0.53681253355757641, 'Training Accuracy', 100.0)\n",
      "('Epoch', 643, 'Loss', 0.53610356517973234, 'Training Accuracy', 100.0)\n",
      "('Epoch', 644, 'Loss', 0.53539654163602057, 'Training Accuracy', 100.0)\n",
      "('Epoch', 645, 'Loss', 0.53469145485320102, 'Training Accuracy', 100.0)\n",
      "('Epoch', 646, 'Loss', 0.53398829680283388, 'Training Accuracy', 100.0)\n",
      "('Epoch', 647, 'Loss', 0.53328705950098465, 'Training Accuracy', 100.0)\n",
      "('Epoch', 648, 'Loss', 0.5325877350079079, 'Training Accuracy', 100.0)\n",
      "('Epoch', 649, 'Loss', 0.53189031542774179, 'Training Accuracy', 100.0)\n",
      "('Epoch', 650, 'Loss', 0.53119479290820149, 'Training Accuracy', 100.0)\n",
      "('Epoch', 651, 'Loss', 0.53050115964029143, 'Training Accuracy', 100.0)\n",
      "('Epoch', 652, 'Loss', 0.5298094078579878, 'Training Accuracy', 100.0)\n",
      "('Epoch', 653, 'Loss', 0.52911952983796107, 'Training Accuracy', 100.0)\n",
      "('Epoch', 654, 'Loss', 0.52843151789926546, 'Training Accuracy', 100.0)\n",
      "('Epoch', 655, 'Loss', 0.52774536440306119, 'Training Accuracy', 100.0)\n",
      "('Epoch', 656, 'Loss', 0.52706106175231804, 'Training Accuracy', 100.0)\n",
      "('Epoch', 657, 'Loss', 0.52637860239153289, 'Training Accuracy', 100.0)\n",
      "('Epoch', 658, 'Loss', 0.52569797880644786, 'Training Accuracy', 100.0)\n",
      "('Epoch', 659, 'Loss', 0.52501918352375809, 'Training Accuracy', 100.0)\n",
      "('Epoch', 660, 'Loss', 0.52434220911084306, 'Training Accuracy', 100.0)\n",
      "('Epoch', 661, 'Loss', 0.52366704817549037, 'Training Accuracy', 100.0)\n",
      "('Epoch', 662, 'Loss', 0.52299369336560653, 'Training Accuracy', 100.0)\n",
      "('Epoch', 663, 'Loss', 0.52232213736896849, 'Training Accuracy', 100.0)\n",
      "('Epoch', 664, 'Loss', 0.52165237291292532, 'Training Accuracy', 100.0)\n",
      "('Epoch', 665, 'Loss', 0.52098439276415343, 'Training Accuracy', 100.0)\n",
      "('Epoch', 666, 'Loss', 0.52031818972837762, 'Training Accuracy', 100.0)\n",
      "('Epoch', 667, 'Loss', 0.51965375665011115, 'Training Accuracy', 100.0)\n",
      "('Epoch', 668, 'Loss', 0.51899108641239755, 'Training Accuracy', 100.0)\n",
      "('Epoch', 669, 'Loss', 0.51833017193654407, 'Training Accuracy', 100.0)\n",
      "('Epoch', 670, 'Loss', 0.5176710061818669, 'Training Accuracy', 100.0)\n",
      "('Epoch', 671, 'Loss', 0.51701358214543747, 'Training Accuracy', 100.0)\n",
      "('Epoch', 672, 'Loss', 0.51635789286183165, 'Training Accuracy', 100.0)\n",
      "('Epoch', 673, 'Loss', 0.51570393140287107, 'Training Accuracy', 100.0)\n",
      "('Epoch', 674, 'Loss', 0.51505169087738012, 'Training Accuracy', 100.0)\n",
      "('Epoch', 675, 'Loss', 0.51440116443093686, 'Training Accuracy', 100.0)\n",
      "('Epoch', 676, 'Loss', 0.51375234524562707, 'Training Accuracy', 100.0)\n",
      "('Epoch', 677, 'Loss', 0.51310522653979718, 'Training Accuracy', 100.0)\n",
      "('Epoch', 678, 'Loss', 0.51245980156782389, 'Training Accuracy', 100.0)\n",
      "('Epoch', 679, 'Loss', 0.51181606361986898, 'Training Accuracy', 100.0)\n",
      "('Epoch', 680, 'Loss', 0.51117400602163088, 'Training Accuracy', 100.0)\n",
      "('Epoch', 681, 'Loss', 0.51053362213412812, 'Training Accuracy', 100.0)\n",
      "('Epoch', 682, 'Loss', 0.50989490535345572, 'Training Accuracy', 100.0)\n",
      "('Epoch', 683, 'Loss', 0.50925784911055061, 'Training Accuracy', 100.0)\n",
      "('Epoch', 684, 'Loss', 0.50862244687096736, 'Training Accuracy', 100.0)\n",
      "('Epoch', 685, 'Loss', 0.50798869213464493, 'Training Accuracy', 100.0)\n",
      "('Epoch', 686, 'Loss', 0.5073565784356896, 'Training Accuracy', 100.0)\n",
      "('Epoch', 687, 'Loss', 0.50672609934213464, 'Training Accuracy', 100.0)\n",
      "('Epoch', 688, 'Loss', 0.50609724845573423, 'Training Accuracy', 100.0)\n",
      "('Epoch', 689, 'Loss', 0.50547001941173431, 'Training Accuracy', 100.0)\n",
      "('Epoch', 690, 'Loss', 0.50484440587864798, 'Training Accuracy', 100.0)\n",
      "('Epoch', 691, 'Loss', 0.50422040155804715, 'Training Accuracy', 100.0)\n",
      "('Epoch', 692, 'Loss', 0.50359800018434608, 'Training Accuracy', 100.0)\n",
      "('Epoch', 693, 'Loss', 0.50297719552457787, 'Training Accuracy', 100.0)\n",
      "('Epoch', 694, 'Loss', 0.50235798137819232, 'Training Accuracy', 100.0)\n",
      "('Epoch', 695, 'Loss', 0.50174035157684171, 'Training Accuracy', 100.0)\n",
      "('Epoch', 696, 'Loss', 0.50112429998416552, 'Training Accuracy', 100.0)\n",
      "('Epoch', 697, 'Loss', 0.50050982049559478, 'Training Accuracy', 100.0)\n",
      "('Epoch', 698, 'Loss', 0.49989690703813622, 'Training Accuracy', 100.0)\n",
      "('Epoch', 699, 'Loss', 0.49928555357016885, 'Training Accuracy', 100.0)\n",
      "('Epoch', 700, 'Loss', 0.49867575408124548, 'Training Accuracy', 100.0)\n",
      "('Epoch', 701, 'Loss', 0.49806750259189064, 'Training Accuracy', 100.0)\n",
      "('Epoch', 702, 'Loss', 0.49746079315339825, 'Training Accuracy', 100.0)\n",
      "('Epoch', 703, 'Loss', 0.49685561984763305, 'Training Accuracy', 100.0)\n",
      "('Epoch', 704, 'Loss', 0.49625197678683924, 'Training Accuracy', 100.0)\n",
      "('Epoch', 705, 'Loss', 0.49564985811343953, 'Training Accuracy', 100.0)\n",
      "('Epoch', 706, 'Loss', 0.49504925799984506, 'Training Accuracy', 100.0)\n",
      "('Epoch', 707, 'Loss', 0.49445017064826297, 'Training Accuracy', 100.0)\n",
      "('Epoch', 708, 'Loss', 0.49385259029050321, 'Training Accuracy', 100.0)\n",
      "('Epoch', 709, 'Loss', 0.49325651118779001, 'Training Accuracy', 100.0)\n",
      "('Epoch', 710, 'Loss', 0.49266192763058342, 'Training Accuracy', 100.0)\n",
      "('Epoch', 711, 'Loss', 0.49206883393837297, 'Training Accuracy', 100.0)\n",
      "('Epoch', 712, 'Loss', 0.49147722445951442, 'Training Accuracy', 100.0)\n",
      "('Epoch', 713, 'Loss', 0.49088709357103588, 'Training Accuracy', 100.0)\n",
      "('Epoch', 714, 'Loss', 0.49029843567845116, 'Training Accuracy', 100.0)\n",
      "('Epoch', 715, 'Loss', 0.48971124521559051, 'Training Accuracy', 100.0)\n",
      "('Epoch', 716, 'Loss', 0.48912551664441062, 'Training Accuracy', 100.0)\n",
      "('Epoch', 717, 'Loss', 0.48854124445482705, 'Training Accuracy', 100.0)\n",
      "('Epoch', 718, 'Loss', 0.48795842316452986, 'Training Accuracy', 100.0)\n",
      "('Epoch', 719, 'Loss', 0.48737704731881099, 'Training Accuracy', 100.0)\n",
      "('Epoch', 720, 'Loss', 0.48679711149039007, 'Training Accuracy', 100.0)\n",
      "('Epoch', 721, 'Loss', 0.48621861027924057, 'Training Accuracy', 100.0)\n",
      "('Epoch', 722, 'Loss', 0.48564153831242191, 'Training Accuracy', 100.0)\n",
      "('Epoch', 723, 'Loss', 0.48506589024390867, 'Training Accuracy', 100.0)\n",
      "('Epoch', 724, 'Loss', 0.48449166075442113, 'Training Accuracy', 100.0)\n",
      "('Epoch', 725, 'Loss', 0.48391884455125334, 'Training Accuracy', 100.0)\n",
      "('Epoch', 726, 'Loss', 0.4833474363681135, 'Training Accuracy', 100.0)\n",
      "('Epoch', 727, 'Loss', 0.48277743096496029, 'Training Accuracy', 100.0)\n",
      "('Epoch', 728, 'Loss', 0.4822088231278277, 'Training Accuracy', 100.0)\n",
      "('Epoch', 729, 'Loss', 0.48164160766868142, 'Training Accuracy', 100.0)\n",
      "('Epoch', 730, 'Loss', 0.4810757794252345, 'Training Accuracy', 100.0)\n",
      "('Epoch', 731, 'Loss', 0.48051133326081019, 'Training Accuracy', 100.0)\n",
      "('Epoch', 732, 'Loss', 0.47994826406416363, 'Training Accuracy', 100.0)\n",
      "('Epoch', 733, 'Loss', 0.47938656674933899, 'Training Accuracy', 100.0)\n",
      "('Epoch', 734, 'Loss', 0.4788262362555063, 'Training Accuracy', 100.0)\n",
      "('Epoch', 735, 'Loss', 0.4782672675468016, 'Training Accuracy', 100.0)\n",
      "('Epoch', 736, 'Loss', 0.47770965561218454, 'Training Accuracy', 100.0)\n",
      "('Epoch', 737, 'Loss', 0.47715339546527558, 'Training Accuracy', 100.0)\n",
      "('Epoch', 738, 'Loss', 0.4765984821442083, 'Training Accuracy', 100.0)\n",
      "('Epoch', 739, 'Loss', 0.4760449107114747, 'Training Accuracy', 100.0)\n",
      "('Epoch', 740, 'Loss', 0.47549267625378183, 'Training Accuracy', 100.0)\n",
      "('Epoch', 741, 'Loss', 0.47494177388189801, 'Training Accuracy', 100.0)\n",
      "('Epoch', 742, 'Loss', 0.47439219873050781, 'Training Accuracy', 100.0)\n",
      "('Epoch', 743, 'Loss', 0.47384394595806406, 'Training Accuracy', 100.0)\n",
      "('Epoch', 744, 'Loss', 0.47329701074664715, 'Training Accuracy', 100.0)\n",
      "('Epoch', 745, 'Loss', 0.47275138830180824, 'Training Accuracy', 100.0)\n",
      "('Epoch', 746, 'Loss', 0.4722070738524462, 'Training Accuracy', 100.0)\n",
      "('Epoch', 747, 'Loss', 0.47166406265064842, 'Training Accuracy', 100.0)\n",
      "('Epoch', 748, 'Loss', 0.47112234997155311, 'Training Accuracy', 100.0)\n",
      "('Epoch', 749, 'Loss', 0.47058193111321933, 'Training Accuracy', 100.0)\n",
      "('Epoch', 750, 'Loss', 0.4700428013964732, 'Training Accuracy', 100.0)\n",
      "('Epoch', 751, 'Loss', 0.4695049561647765, 'Training Accuracy', 100.0)\n",
      "('Epoch', 752, 'Loss', 0.4689683907840978, 'Training Accuracy', 100.0)\n",
      "('Epoch', 753, 'Loss', 0.46843310064275978, 'Training Accuracy', 100.0)\n",
      "('Epoch', 754, 'Loss', 0.46789908115131956, 'Training Accuracy', 100.0)\n",
      "('Epoch', 755, 'Loss', 0.46736632774242637, 'Training Accuracy', 100.0)\n",
      "('Epoch', 756, 'Loss', 0.46683483587068575, 'Training Accuracy', 100.0)\n",
      "('Epoch', 757, 'Loss', 0.46630460101254056, 'Training Accuracy', 100.0)\n",
      "('Epoch', 758, 'Loss', 0.4657756186661251, 'Training Accuracy', 100.0)\n",
      "('Epoch', 759, 'Loss', 0.46524788435113934, 'Training Accuracy', 100.0)\n",
      "('Epoch', 760, 'Loss', 0.46472139360872733, 'Training Accuracy', 100.0)\n",
      "('Epoch', 761, 'Loss', 0.46419614200134035, 'Training Accuracy', 100.0)\n",
      "('Epoch', 762, 'Loss', 0.46367212511260897, 'Training Accuracy', 100.0)\n",
      "('Epoch', 763, 'Loss', 0.46314933854722201, 'Training Accuracy', 100.0)\n",
      "('Epoch', 764, 'Loss', 0.4626277779307994, 'Training Accuracy', 100.0)\n",
      "('Epoch', 765, 'Loss', 0.46210743890976314, 'Training Accuracy', 100.0)\n",
      "('Epoch', 766, 'Loss', 0.46158831715121956, 'Training Accuracy', 100.0)\n",
      "('Epoch', 767, 'Loss', 0.46107040834283158, 'Training Accuracy', 100.0)\n",
      "('Epoch', 768, 'Loss', 0.46055370819270003, 'Training Accuracy', 100.0)\n",
      "('Epoch', 769, 'Loss', 0.46003821242924142, 'Training Accuracy', 100.0)\n",
      "('Epoch', 770, 'Loss', 0.45952391680106436, 'Training Accuracy', 100.0)\n",
      "('Epoch', 771, 'Loss', 0.45901081707685687, 'Training Accuracy', 100.0)\n",
      "('Epoch', 772, 'Loss', 0.45849890904525925, 'Training Accuracy', 100.0)\n",
      "('Epoch', 773, 'Loss', 0.45798818851475476, 'Training Accuracy', 100.0)\n",
      "('Epoch', 774, 'Loss', 0.45747865131354759, 'Training Accuracy', 100.0)\n",
      "('Epoch', 775, 'Loss', 0.45697029328944788, 'Training Accuracy', 100.0)\n",
      "('Epoch', 776, 'Loss', 0.45646311030975406, 'Training Accuracy', 100.0)\n",
      "('Epoch', 777, 'Loss', 0.45595709826114622, 'Training Accuracy', 100.0)\n",
      "('Epoch', 778, 'Loss', 0.45545225304955805, 'Training Accuracy', 100.0)\n",
      "('Epoch', 779, 'Loss', 0.45494857060008415, 'Training Accuracy', 100.0)\n",
      "('Epoch', 780, 'Loss', 0.45444604685684825, 'Training Accuracy', 100.0)\n",
      "('Epoch', 781, 'Loss', 0.45394467778290387, 'Training Accuracy', 100.0)\n",
      "('Epoch', 782, 'Loss', 0.45344445936011551, 'Training Accuracy', 100.0)\n",
      "('Epoch', 783, 'Loss', 0.45294538758906117, 'Training Accuracy', 100.0)\n",
      "('Epoch', 784, 'Loss', 0.45244745848891255, 'Training Accuracy', 100.0)\n",
      "('Epoch', 785, 'Loss', 0.45195066809732354, 'Training Accuracy', 100.0)\n",
      "('Epoch', 786, 'Loss', 0.45145501247033887, 'Training Accuracy', 100.0)\n",
      "('Epoch', 787, 'Loss', 0.45096048768227537, 'Training Accuracy', 100.0)\n",
      "('Epoch', 788, 'Loss', 0.45046708982561517, 'Training Accuracy', 100.0)\n",
      "('Epoch', 789, 'Loss', 0.44997481501090708, 'Training Accuracy', 100.0)\n",
      "('Epoch', 790, 'Loss', 0.44948365936665541, 'Training Accuracy', 100.0)\n",
      "('Epoch', 791, 'Loss', 0.44899361903922208, 'Training Accuracy', 100.0)\n",
      "('Epoch', 792, 'Loss', 0.44850469019272188, 'Training Accuracy', 100.0)\n",
      "('Epoch', 793, 'Loss', 0.4480168690089178, 'Training Accuracy', 100.0)\n",
      "('Epoch', 794, 'Loss', 0.44753015168711852, 'Training Accuracy', 100.0)\n",
      "('Epoch', 795, 'Loss', 0.44704453444408448, 'Training Accuracy', 100.0)\n",
      "('Epoch', 796, 'Loss', 0.44656001351392366, 'Training Accuracy', 100.0)\n",
      "('Epoch', 797, 'Loss', 0.44607658514798254, 'Training Accuracy', 100.0)\n",
      "('Epoch', 798, 'Loss', 0.44559424561476535, 'Training Accuracy', 100.0)\n",
      "('Epoch', 799, 'Loss', 0.44511299119982362, 'Training Accuracy', 100.0)\n",
      "('Epoch', 800, 'Loss', 0.44463281820566003, 'Training Accuracy', 100.0)\n",
      "('Epoch', 801, 'Loss', 0.44415372295163674, 'Training Accuracy', 100.0)\n",
      "('Epoch', 802, 'Loss', 0.443675701773874, 'Training Accuracy', 100.0)\n",
      "('Epoch', 803, 'Loss', 0.44319875102515116, 'Training Accuracy', 100.0)\n",
      "('Epoch', 804, 'Loss', 0.442722867074828, 'Training Accuracy', 100.0)\n",
      "('Epoch', 805, 'Loss', 0.44224804630873121, 'Training Accuracy', 100.0)\n",
      "('Epoch', 806, 'Loss', 0.4417742851290708, 'Training Accuracy', 100.0)\n",
      "('Epoch', 807, 'Loss', 0.44130157995434782, 'Training Accuracy', 100.0)\n",
      "('Epoch', 808, 'Loss', 0.44082992721925546, 'Training Accuracy', 100.0)\n",
      "('Epoch', 809, 'Loss', 0.44035932337459777, 'Training Accuracy', 100.0)\n",
      "('Epoch', 810, 'Loss', 0.43988976488718723, 'Training Accuracy', 100.0)\n",
      "('Epoch', 811, 'Loss', 0.43942124823976136, 'Training Accuracy', 100.0)\n",
      "('Epoch', 812, 'Loss', 0.43895376993089275, 'Training Accuracy', 100.0)\n",
      "('Epoch', 813, 'Loss', 0.43848732647489619, 'Training Accuracy', 100.0)\n",
      "('Epoch', 814, 'Loss', 0.43802191440174143, 'Training Accuracy', 100.0)\n",
      "('Epoch', 815, 'Loss', 0.43755753025697142, 'Training Accuracy', 100.0)\n",
      "('Epoch', 816, 'Loss', 0.43709417060160261, 'Training Accuracy', 100.0)\n",
      "('Epoch', 817, 'Loss', 0.43663183201204653, 'Training Accuracy', 100.0)\n",
      "('Epoch', 818, 'Loss', 0.4361705110800263, 'Training Accuracy', 100.0)\n",
      "('Epoch', 819, 'Loss', 0.43571020441248198, 'Training Accuracy', 100.0)\n",
      "('Epoch', 820, 'Loss', 0.43525090863149124, 'Training Accuracy', 100.0)\n",
      "('Epoch', 821, 'Loss', 0.43479262037418526, 'Training Accuracy', 100.0)\n",
      "('Epoch', 822, 'Loss', 0.43433533629265891, 'Training Accuracy', 100.0)\n",
      "('Epoch', 823, 'Loss', 0.43387905305389968, 'Training Accuracy', 100.0)\n",
      "('Epoch', 824, 'Loss', 0.43342376733968996, 'Training Accuracy', 100.0)\n",
      "('Epoch', 825, 'Loss', 0.43296947584653256, 'Training Accuracy', 100.0)\n",
      "('Epoch', 826, 'Loss', 0.43251617528557174, 'Training Accuracy', 100.0)\n",
      "('Epoch', 827, 'Loss', 0.43206386238250349, 'Training Accuracy', 100.0)\n",
      "('Epoch', 828, 'Loss', 0.43161253387750537, 'Training Accuracy', 100.0)\n",
      "('Epoch', 829, 'Loss', 0.43116218652514421, 'Training Accuracy', 100.0)\n",
      "('Epoch', 830, 'Loss', 0.43071281709431364, 'Training Accuracy', 100.0)\n",
      "('Epoch', 831, 'Loss', 0.43026442236812928, 'Training Accuracy', 100.0)\n",
      "('Epoch', 832, 'Loss', 0.42981699914388088, 'Training Accuracy', 100.0)\n",
      "('Epoch', 833, 'Loss', 0.4293705442329297, 'Training Accuracy', 100.0)\n",
      "('Epoch', 834, 'Loss', 0.42892505446064277, 'Training Accuracy', 100.0)\n",
      "('Epoch', 835, 'Loss', 0.42848052666631403, 'Training Accuracy', 100.0)\n",
      "('Epoch', 836, 'Loss', 0.42803695770308714, 'Training Accuracy', 100.0)\n",
      "('Epoch', 837, 'Loss', 0.42759434443787797, 'Training Accuracy', 100.0)\n",
      "('Epoch', 838, 'Loss', 0.42715268375130422, 'Training Accuracy', 100.0)\n",
      "('Epoch', 839, 'Loss', 0.42671197253760423, 'Training Accuracy', 100.0)\n",
      "('Epoch', 840, 'Loss', 0.42627220770456792, 'Training Accuracy', 100.0)\n",
      "('Epoch', 841, 'Loss', 0.42583338617345551, 'Training Accuracy', 100.0)\n",
      "('Epoch', 842, 'Loss', 0.42539550487893552, 'Training Accuracy', 100.0)\n",
      "('Epoch', 843, 'Loss', 0.42495856076899702, 'Training Accuracy', 100.0)\n",
      "('Epoch', 844, 'Loss', 0.42452255080489437, 'Training Accuracy', 100.0)\n",
      "('Epoch', 845, 'Loss', 0.42408747196105817, 'Training Accuracy', 100.0)\n",
      "('Epoch', 846, 'Loss', 0.42365332122503552, 'Training Accuracy', 100.0)\n",
      "('Epoch', 847, 'Loss', 0.42322009559741158, 'Training Accuracy', 100.0)\n",
      "('Epoch', 848, 'Loss', 0.42278779209174772, 'Training Accuracy', 100.0)\n",
      "('Epoch', 849, 'Loss', 0.42235640773449712, 'Training Accuracy', 100.0)\n",
      "('Epoch', 850, 'Loss', 0.42192593956495172, 'Training Accuracy', 100.0)\n",
      "('Epoch', 851, 'Loss', 0.42149638463515965, 'Training Accuracy', 100.0)\n",
      "('Epoch', 852, 'Loss', 0.42106774000986547, 'Training Accuracy', 100.0)\n",
      "('Epoch', 853, 'Loss', 0.42064000276643243, 'Training Accuracy', 100.0)\n",
      "('Epoch', 854, 'Loss', 0.42021316999478542, 'Training Accuracy', 100.0)\n",
      "('Epoch', 855, 'Loss', 0.41978723879733654, 'Training Accuracy', 100.0)\n",
      "('Epoch', 856, 'Loss', 0.41936220628891491, 'Training Accuracy', 100.0)\n",
      "('Epoch', 857, 'Loss', 0.41893806959670932, 'Training Accuracy', 100.0)\n",
      "('Epoch', 858, 'Loss', 0.41851482586019961, 'Training Accuracy', 100.0)\n",
      "('Epoch', 859, 'Loss', 0.41809247223107909, 'Training Accuracy', 100.0)\n",
      "('Epoch', 860, 'Loss', 0.41767100587320732, 'Training Accuracy', 100.0)\n",
      "('Epoch', 861, 'Loss', 0.41725042396253353, 'Training Accuracy', 100.0)\n",
      "('Epoch', 862, 'Loss', 0.41683072368703239, 'Training Accuracy', 100.0)\n",
      "('Epoch', 863, 'Loss', 0.41641190224664848, 'Training Accuracy', 100.0)\n",
      "('Epoch', 864, 'Loss', 0.41599395685322038, 'Training Accuracy', 100.0)\n",
      "('Epoch', 865, 'Loss', 0.4155768847304262, 'Training Accuracy', 100.0)\n",
      "('Epoch', 866, 'Loss', 0.41516068311371662, 'Training Accuracy', 100.0)\n",
      "('Epoch', 867, 'Loss', 0.41474534925025636, 'Training Accuracy', 100.0)\n",
      "('Epoch', 868, 'Loss', 0.41433088039885685, 'Training Accuracy', 100.0)\n",
      "('Epoch', 869, 'Loss', 0.41391727382992044, 'Training Accuracy', 100.0)\n",
      "('Epoch', 870, 'Loss', 0.41350452682537309, 'Training Accuracy', 100.0)\n",
      "('Epoch', 871, 'Loss', 0.41309263667860485, 'Training Accuracy', 100.0)\n",
      "('Epoch', 872, 'Loss', 0.41268160069441451, 'Training Accuracy', 100.0)\n",
      "('Epoch', 873, 'Loss', 0.41227141618894281, 'Training Accuracy', 100.0)\n",
      "('Epoch', 874, 'Loss', 0.41186208048961753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 875, 'Loss', 0.41145359093508804, 'Training Accuracy', 100.0)\n",
      "('Epoch', 876, 'Loss', 0.41104594487517504, 'Training Accuracy', 100.0)\n",
      "('Epoch', 877, 'Loss', 0.41063913967080401, 'Training Accuracy', 100.0)\n",
      "('Epoch', 878, 'Loss', 0.41023317269395104, 'Training Accuracy', 100.0)\n",
      "('Epoch', 879, 'Loss', 0.40982804132758566, 'Training Accuracy', 100.0)\n",
      "('Epoch', 880, 'Loss', 0.40942374296560957, 'Training Accuracy', 100.0)\n",
      "('Epoch', 881, 'Loss', 0.40902027501279864, 'Training Accuracy', 100.0)\n",
      "('Epoch', 882, 'Loss', 0.40861763488475505, 'Training Accuracy', 100.0)\n",
      "('Epoch', 883, 'Loss', 0.40821582000783979, 'Training Accuracy', 100.0)\n",
      "('Epoch', 884, 'Loss', 0.40781482781912426, 'Training Accuracy', 100.0)\n",
      "('Epoch', 885, 'Loss', 0.40741465576632907, 'Training Accuracy', 100.0)\n",
      "('Epoch', 886, 'Loss', 0.40701530130777469, 'Training Accuracy', 100.0)\n",
      "('Epoch', 887, 'Loss', 0.40661676191231949, 'Training Accuracy', 100.0)\n",
      "('Epoch', 888, 'Loss', 0.40621903505931023, 'Training Accuracy', 100.0)\n",
      "('Epoch', 889, 'Loss', 0.40582211823852454, 'Training Accuracy', 100.0)\n",
      "('Epoch', 890, 'Loss', 0.40542600895012182, 'Training Accuracy', 100.0)\n",
      "('Epoch', 891, 'Loss', 0.40503070470458069, 'Training Accuracy', 100.0)\n",
      "('Epoch', 892, 'Loss', 0.40463620302265446, 'Training Accuracy', 100.0)\n",
      "('Epoch', 893, 'Loss', 0.40424250143531182, 'Training Accuracy', 100.0)\n",
      "('Epoch', 894, 'Loss', 0.40384959748369037, 'Training Accuracy', 100.0)\n",
      "('Epoch', 895, 'Loss', 0.40345748871903842, 'Training Accuracy', 100.0)\n",
      "('Epoch', 896, 'Loss', 0.40306617270266504, 'Training Accuracy', 100.0)\n",
      "('Epoch', 897, 'Loss', 0.40267564700588737, 'Training Accuracy', 100.0)\n",
      "('Epoch', 898, 'Loss', 0.40228590920998253, 'Training Accuracy', 100.0)\n",
      "('Epoch', 899, 'Loss', 0.40189695690613558, 'Training Accuracy', 100.0)\n",
      "('Epoch', 900, 'Loss', 0.40150878769538212, 'Training Accuracy', 100.0)\n",
      "('Epoch', 901, 'Loss', 0.4011213991885666, 'Training Accuracy', 100.0)\n",
      "('Epoch', 902, 'Loss', 0.40073478900628795, 'Training Accuracy', 100.0)\n",
      "('Epoch', 903, 'Loss', 0.40034895477885329, 'Training Accuracy', 100.0)\n",
      "('Epoch', 904, 'Loss', 0.39996389414621764, 'Training Accuracy', 100.0)\n",
      "('Epoch', 905, 'Loss', 0.39957960475795101, 'Training Accuracy', 100.0)\n",
      "('Epoch', 906, 'Loss', 0.39919608427317799, 'Training Accuracy', 100.0)\n",
      "('Epoch', 907, 'Loss', 0.39881333036052746, 'Training Accuracy', 100.0)\n",
      "('Epoch', 908, 'Loss', 0.39843134069809683, 'Training Accuracy', 100.0)\n",
      "('Epoch', 909, 'Loss', 0.39805011297339316, 'Training Accuracy', 100.0)\n",
      "('Epoch', 910, 'Loss', 0.3976696448832811, 'Training Accuracy', 100.0)\n",
      "('Epoch', 911, 'Loss', 0.39728993413395247, 'Training Accuracy', 100.0)\n",
      "('Epoch', 912, 'Loss', 0.39691097844086698, 'Training Accuracy', 100.0)\n",
      "('Epoch', 913, 'Loss', 0.39653277552870225, 'Training Accuracy', 100.0)\n",
      "('Epoch', 914, 'Loss', 0.39615532313131652, 'Training Accuracy', 100.0)\n",
      "('Epoch', 915, 'Loss', 0.39577861899170058, 'Training Accuracy', 100.0)\n",
      "('Epoch', 916, 'Loss', 0.39540266086192305, 'Training Accuracy', 100.0)\n",
      "('Epoch', 917, 'Loss', 0.39502744650309302, 'Training Accuracy', 100.0)\n",
      "('Epoch', 918, 'Loss', 0.39465297368531849, 'Training Accuracy', 100.0)\n",
      "('Epoch', 919, 'Loss', 0.39427924018764748, 'Training Accuracy', 100.0)\n",
      "('Epoch', 920, 'Loss', 0.39390624379803363, 'Training Accuracy', 100.0)\n",
      "('Epoch', 921, 'Loss', 0.39353398231328895, 'Training Accuracy', 100.0)\n",
      "('Epoch', 922, 'Loss', 0.39316245353904045, 'Training Accuracy', 100.0)\n",
      "('Epoch', 923, 'Loss', 0.39279165528968069, 'Training Accuracy', 100.0)\n",
      "('Epoch', 924, 'Loss', 0.39242158538833122, 'Training Accuracy', 100.0)\n",
      "('Epoch', 925, 'Loss', 0.39205224166679237, 'Training Accuracy', 100.0)\n",
      "('Epoch', 926, 'Loss', 0.39168362196551204, 'Training Accuracy', 100.0)\n",
      "('Epoch', 927, 'Loss', 0.39131572413351906, 'Training Accuracy', 100.0)\n",
      "('Epoch', 928, 'Loss', 0.39094854602840695, 'Training Accuracy', 100.0)\n",
      "('Epoch', 929, 'Loss', 0.39058208551627682, 'Training Accuracy', 100.0)\n",
      "('Epoch', 930, 'Loss', 0.39021634047169335, 'Training Accuracy', 100.0)\n",
      "('Epoch', 931, 'Loss', 0.38985130877765106, 'Training Accuracy', 100.0)\n",
      "('Epoch', 932, 'Loss', 0.38948698832552842, 'Training Accuracy', 100.0)\n",
      "('Epoch', 933, 'Loss', 0.38912337701504368, 'Training Accuracy', 100.0)\n",
      "('Epoch', 934, 'Loss', 0.38876047275421888, 'Training Accuracy', 100.0)\n",
      "('Epoch', 935, 'Loss', 0.38839827345933631, 'Training Accuracy', 100.0)\n",
      "('Epoch', 936, 'Loss', 0.38803677705489309, 'Training Accuracy', 100.0)\n",
      "('Epoch', 937, 'Loss', 0.38767598147357057, 'Training Accuracy', 100.0)\n",
      "('Epoch', 938, 'Loss', 0.38731588465618505, 'Training Accuracy', 100.0)\n",
      "('Epoch', 939, 'Loss', 0.38695648455165327, 'Training Accuracy', 100.0)\n",
      "('Epoch', 940, 'Loss', 0.38659777911694515, 'Training Accuracy', 100.0)\n",
      "('Epoch', 941, 'Loss', 0.38623976631705592, 'Training Accuracy', 100.0)\n",
      "('Epoch', 942, 'Loss', 0.38588244412495709, 'Training Accuracy', 100.0)\n",
      "('Epoch', 943, 'Loss', 0.38552581052155965, 'Training Accuracy', 100.0)\n",
      "('Epoch', 944, 'Loss', 0.38516986349567517, 'Training Accuracy', 100.0)\n",
      "('Epoch', 945, 'Loss', 0.38481460104398019, 'Training Accuracy', 100.0)\n",
      "('Epoch', 946, 'Loss', 0.38446002117097328, 'Training Accuracy', 100.0)\n",
      "('Epoch', 947, 'Loss', 0.38410612188894233, 'Training Accuracy', 100.0)\n",
      "('Epoch', 948, 'Loss', 0.383752901217915, 'Training Accuracy', 100.0)\n",
      "('Epoch', 949, 'Loss', 0.38340035718563581, 'Training Accuracy', 100.0)\n",
      "('Epoch', 950, 'Loss', 0.38304848782752032, 'Training Accuracy', 100.0)\n",
      "('Epoch', 951, 'Loss', 0.38269729118661755, 'Training Accuracy', 100.0)\n",
      "('Epoch', 952, 'Loss', 0.38234676531357475, 'Training Accuracy', 100.0)\n",
      "('Epoch', 953, 'Loss', 0.38199690826659854, 'Training Accuracy', 100.0)\n",
      "('Epoch', 954, 'Loss', 0.38164771811142517, 'Training Accuracy', 100.0)\n",
      "('Epoch', 955, 'Loss', 0.38129919292127268, 'Training Accuracy', 100.0)\n",
      "('Epoch', 956, 'Loss', 0.38095133077681104, 'Training Accuracy', 100.0)\n",
      "('Epoch', 957, 'Loss', 0.38060412976612756, 'Training Accuracy', 100.0)\n",
      "('Epoch', 958, 'Loss', 0.38025758798469, 'Training Accuracy', 100.0)\n",
      "('Epoch', 959, 'Loss', 0.37991170353530518, 'Training Accuracy', 100.0)\n",
      "('Epoch', 960, 'Loss', 0.37956647452809333, 'Training Accuracy', 100.0)\n",
      "('Epoch', 961, 'Loss', 0.37922189908044784, 'Training Accuracy', 100.0)\n",
      "('Epoch', 962, 'Loss', 0.37887797531699291, 'Training Accuracy', 100.0)\n",
      "('Epoch', 963, 'Loss', 0.37853470136956369, 'Training Accuracy', 100.0)\n",
      "('Epoch', 964, 'Loss', 0.37819207537715993, 'Training Accuracy', 100.0)\n",
      "('Epoch', 965, 'Loss', 0.37785009548591808, 'Training Accuracy', 100.0)\n",
      "('Epoch', 966, 'Loss', 0.37750875984907184, 'Training Accuracy', 100.0)\n",
      "('Epoch', 967, 'Loss', 0.3771680666269196, 'Training Accuracy', 100.0)\n",
      "('Epoch', 968, 'Loss', 0.37682801398679844, 'Training Accuracy', 100.0)\n",
      "('Epoch', 969, 'Loss', 0.37648860010303659, 'Training Accuracy', 100.0)\n",
      "('Epoch', 970, 'Loss', 0.37614982315692991, 'Training Accuracy', 100.0)\n",
      "('Epoch', 971, 'Loss', 0.37581168133670478, 'Training Accuracy', 100.0)\n",
      "('Epoch', 972, 'Loss', 0.37547417283748918, 'Training Accuracy', 100.0)\n",
      "('Epoch', 973, 'Loss', 0.37513729586127192, 'Training Accuracy', 100.0)\n",
      "('Epoch', 974, 'Loss', 0.37480104861688096, 'Training Accuracy', 100.0)\n",
      "('Epoch', 975, 'Loss', 0.37446542931993776, 'Training Accuracy', 100.0)\n",
      "('Epoch', 976, 'Loss', 0.37413043619283609, 'Training Accuracy', 100.0)\n",
      "('Epoch', 977, 'Loss', 0.37379606746470567, 'Training Accuracy', 100.0)\n",
      "('Epoch', 978, 'Loss', 0.37346232137137658, 'Training Accuracy', 100.0)\n",
      "('Epoch', 979, 'Loss', 0.37312919615535833, 'Training Accuracy', 100.0)\n",
      "('Epoch', 980, 'Loss', 0.37279669006579325, 'Training Accuracy', 100.0)\n",
      "('Epoch', 981, 'Loss', 0.3724648013584379, 'Training Accuracy', 100.0)\n",
      "('Epoch', 982, 'Loss', 0.37213352829562285, 'Training Accuracy', 100.0)\n",
      "('Epoch', 983, 'Loss', 0.37180286914623045, 'Training Accuracy', 100.0)\n",
      "('Epoch', 984, 'Loss', 0.37147282218565375, 'Training Accuracy', 100.0)\n",
      "('Epoch', 985, 'Loss', 0.37114338569577771, 'Training Accuracy', 100.0)\n",
      "('Epoch', 986, 'Loss', 0.37081455796493518, 'Training Accuracy', 100.0)\n",
      "('Epoch', 987, 'Loss', 0.37048633728788588, 'Training Accuracy', 100.0)\n",
      "('Epoch', 988, 'Loss', 0.37015872196578659, 'Training Accuracy', 100.0)\n",
      "('Epoch', 989, 'Loss', 0.36983171030615647, 'Training Accuracy', 100.0)\n",
      "('Epoch', 990, 'Loss', 0.36950530062284759, 'Training Accuracy', 100.0)\n",
      "('Epoch', 991, 'Loss', 0.3691794912360169, 'Training Accuracy', 100.0)\n",
      "('Epoch', 992, 'Loss', 0.36885428047209778, 'Training Accuracy', 100.0)\n",
      "('Epoch', 993, 'Loss', 0.36852966666376957, 'Training Accuracy', 100.0)\n",
      "('Epoch', 994, 'Loss', 0.36820564814992812, 'Training Accuracy', 100.0)\n",
      "('Epoch', 995, 'Loss', 0.36788222327565706, 'Training Accuracy', 100.0)\n",
      "('Epoch', 996, 'Loss', 0.36755939039219648, 'Training Accuracy', 100.0)\n",
      "('Epoch', 997, 'Loss', 0.36723714785691958, 'Training Accuracy', 100.0)\n",
      "('Epoch', 998, 'Loss', 0.36691549403329904, 'Training Accuracy', 100.0)\n",
      "('Epoch', 999, 'Loss', 0.36659442729088376, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1000, 'Loss', 0.36627394600526103, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1001, 'Loss', 0.36595404855804109, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1002, 'Loss', 0.36563473333682361, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1003, 'Loss', 0.36531599873516574, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1004, 'Loss', 0.36499784315255857, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1005, 'Loss', 0.36468026499439965, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1006, 'Loss', 0.3643632626719675, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1007, 'Loss', 0.36404683460239473, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1008, 'Loss', 0.36373097920862951, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1009, 'Loss', 0.36341569491942527, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1010, 'Loss', 0.36310098016930747, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1011, 'Loss', 0.36278683339854051, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1012, 'Loss', 0.36247325305311257, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1013, 'Loss', 0.36216023758470162, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1014, 'Loss', 0.36184778545064838, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1015, 'Loss', 0.36153589511394046, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1016, 'Loss', 0.36122456504317585, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1017, 'Loss', 0.36091379371253846, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1018, 'Loss', 0.36060357960178135, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1019, 'Loss', 0.36029392119618725, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1020, 'Loss', 0.35998481698656121, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1021, 'Loss', 0.35967626546918585, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1022, 'Loss', 0.35936826514580972, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1023, 'Loss', 0.35906081452362248, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1024, 'Loss', 0.35875391211521906, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1025, 'Loss', 0.35844755643858961, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1026, 'Loss', 0.35814174601708137, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1027, 'Loss', 0.35783647937938878, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1028, 'Loss', 0.35753175505951151, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1029, 'Loss', 0.35722757159675161, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1030, 'Loss', 0.35692392753566865, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1031, 'Loss', 0.35662082142607066, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1032, 'Loss', 0.35631825182298243, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1033, 'Loss', 0.35601621728663041, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1034, 'Loss', 0.35571471638240826, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1035, 'Loss', 0.35541374768086065, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1036, 'Loss', 0.35511330975766076, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1037, 'Loss', 0.35481340119358479, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1038, 'Loss', 0.35451402057448428, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1039, 'Loss', 0.35421516649127599, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1040, 'Loss', 0.35391683753990927, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1041, 'Loss', 0.35361903232134212, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1042, 'Loss', 0.35332174944152794, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1043, 'Loss', 0.35302498751138744, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1044, 'Loss', 0.35272874514678043, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1045, 'Loss', 0.35243302096850121, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1046, 'Loss', 0.35213781360223428, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1047, 'Loss', 0.3518431216785553, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1048, 'Loss', 0.35154894383288843, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1049, 'Loss', 0.35125527870550072, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1050, 'Loss', 0.350962124941472, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1051, 'Loss', 0.35066948119067615, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1052, 'Loss', 0.35037734610775984, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1053, 'Loss', 0.35008571835212121, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1054, 'Loss', 0.34979459658788747, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1055, 'Loss', 0.34950397948389783, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1056, 'Loss', 0.34921386571367813, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1057, 'Loss', 0.3489242539554252, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1058, 'Loss', 0.34863514289197783, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1059, 'Loss', 0.34834653121080705, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1060, 'Loss', 0.34805841760399292, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1061, 'Loss', 0.34777080076818889, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1062, 'Loss', 0.34748367940463215, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1063, 'Loss', 0.34719705221909231, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1064, 'Loss', 0.34691091792187201, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1065, 'Loss', 0.34662527522778191, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1066, 'Loss', 0.34634012285611376, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1067, 'Loss', 0.34605545953062916, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1068, 'Loss', 0.34577128397953744, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1069, 'Loss', 0.34548759493547798, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1070, 'Loss', 0.34520439113549634, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1071, 'Loss', 0.34492167132102969, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1072, 'Loss', 0.34463943423788379, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1073, 'Loss', 0.34435767863621625, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1074, 'Loss', 0.3440764032705238, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1075, 'Loss', 0.34379560689960525, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1076, 'Loss', 0.3435152882865648, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1077, 'Loss', 0.34323544619877916, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1078, 'Loss', 0.34295607940788198, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1079, 'Loss', 0.34267718668974861, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1080, 'Loss', 0.3423987668244769, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1081, 'Loss', 0.34212081859636473, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1082, 'Loss', 0.34184334079389495, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1083, 'Loss', 0.34156633220972071, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1084, 'Loss', 0.34128979164063999, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1085, 'Loss', 0.341013717887587, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1086, 'Loss', 0.34073810975560381, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1087, 'Loss', 0.34046296605383197, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1088, 'Loss', 0.34018828559548858, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1089, 'Loss', 0.3399140671978545, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1090, 'Loss', 0.3396403096822519, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1091, 'Loss', 0.33936701187402596, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1092, 'Loss', 0.3390941726025366, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1093, 'Loss', 0.3388217907011315, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1094, 'Loss', 0.3385498650071313, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1095, 'Loss', 0.338278394361822, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1096, 'Loss', 0.33800737761041827, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1097, 'Loss', 0.33773681360207003, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1098, 'Loss', 0.33746670118982464, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1099, 'Loss', 0.33719703923063016, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1100, 'Loss', 0.33692782658530201, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1101, 'Loss', 0.33665906211851154, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1102, 'Loss', 0.33639074469878005, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1103, 'Loss', 0.33612287319844497, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1104, 'Loss', 0.33585544649365817, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1105, 'Loss', 0.33558846346435917, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1106, 'Loss', 0.33532192299426827, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1107, 'Loss', 0.33505582397086586, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1108, 'Loss', 0.33479016528537436, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1109, 'Loss', 0.33452494583275261, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1110, 'Loss', 0.33426016451166202, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1111, 'Loss', 0.33399582022446861, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1112, 'Loss', 0.33373191187722218, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1113, 'Loss', 0.33346843837963502, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1114, 'Loss', 0.33320539864506898, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1115, 'Loss', 0.33294279159052992, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1116, 'Loss', 0.33268061613663757, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1117, 'Loss', 0.33241887120761859, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1118, 'Loss', 0.33215755573129185, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1119, 'Loss', 0.33189666863904882, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1120, 'Loss', 0.33163620886584827, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1121, 'Loss', 0.33137617535018465, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1122, 'Loss', 0.3311165670340912, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1123, 'Loss', 0.33085738286311644, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1124, 'Loss', 0.33059862178630456, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1125, 'Loss', 0.3303402827561947, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1126, 'Loss', 0.33008236472879482, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1127, 'Loss', 0.32982486666356986, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1128, 'Loss', 0.32956778752343124, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1129, 'Loss', 0.32931112627471582, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1130, 'Loss', 0.32905488188718346, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1131, 'Loss', 0.32879905333398557, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1132, 'Loss', 0.32854363959166788, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1133, 'Loss', 0.32828863964014593, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1134, 'Loss', 0.32803405246269635, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1135, 'Loss', 0.32777987704593803, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1136, 'Loss', 0.32752611237982582, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1137, 'Loss', 0.32727275745762563, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1138, 'Loss', 0.32701981127591934, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1139, 'Loss', 0.32676727283456436, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1140, 'Loss', 0.32651514113670799, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1141, 'Loss', 0.3262634151887544, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1142, 'Loss', 0.32601209400035908, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1143, 'Loss', 0.32576117658441694, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1144, 'Loss', 0.32551066195704376, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1145, 'Loss', 0.32526054913756564, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1146, 'Loss', 0.32501083714850926, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1147, 'Loss', 0.32476152501558547, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1148, 'Loss', 0.32451261176766971, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1149, 'Loss', 0.32426409643680343, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1150, 'Loss', 0.32401597805817334, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1151, 'Loss', 0.32376825567009188, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1152, 'Loss', 0.32352092831399931, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1153, 'Loss', 0.3232739950344401, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1154, 'Loss', 0.32302745487905343, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1155, 'Loss', 0.32278130689855999, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1156, 'Loss', 0.32253555014675173, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1157, 'Loss', 0.32229018368047524, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1158, 'Loss', 0.32204520655962621, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1159, 'Loss', 0.32180061784712827, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1160, 'Loss', 0.32155641660893008, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1161, 'Loss', 0.32131260191398248, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1162, 'Loss', 0.32106917283423925, 'Training Accuracy', 100.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1163, 'Loss', 0.32082612844463149, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1164, 'Loss', 0.3205834678230679, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1165, 'Loss', 0.32034119005041045, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1166, 'Loss', 0.32009929421047867, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1167, 'Loss', 0.31985777939001803, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1168, 'Loss', 0.31961664467870793, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1169, 'Loss', 0.31937588916913096, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1170, 'Loss', 0.3191355119567747, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1171, 'Loss', 0.31889551214002226, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1172, 'Loss', 0.31865588882012552, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1173, 'Loss', 0.31841664110120732, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1174, 'Loss', 0.31817776809024184, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1175, 'Loss', 0.31793926889705382, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1176, 'Loss', 0.31770114263429283, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1177, 'Loss', 0.31746338841743277, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1178, 'Loss', 0.31722600536475576, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1179, 'Loss', 0.31698899259734492, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1180, 'Loss', 0.3167523492390677, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1181, 'Loss', 0.31651607441656748, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1182, 'Loss', 0.31628016725925756, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1183, 'Loss', 0.3160446268992993, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1184, 'Loss', 0.31580945247160264, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1185, 'Loss', 0.31557464311380795, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1186, 'Loss', 0.31534019796627599, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1187, 'Loss', 0.31510611617207818, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1188, 'Loss', 0.31487239687699037, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1189, 'Loss', 0.31463903922947378, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1190, 'Loss', 0.31440604238067216, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1191, 'Loss', 0.31417340548439282, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1192, 'Loss', 0.31394112769710258, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1193, 'Loss', 0.31370920817792042, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1194, 'Loss', 0.3134776460885994, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1195, 'Loss', 0.31324644059351381, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1196, 'Loss', 0.31301559085966363, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1197, 'Loss', 0.31278509605664689, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1198, 'Loss', 0.31255495535666322, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1199, 'Loss', 0.31232516793449594, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1200, 'Loss', 0.3120957329675017, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1201, 'Loss', 0.31186664963560556, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1202, 'Loss', 0.31163791712128935, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1203, 'Loss', 0.31140953460957682, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1204, 'Loss', 0.31118150128802863, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1205, 'Loss', 0.31095381634673208, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1206, 'Loss', 0.31072647897828959, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1207, 'Loss', 0.31049948837781227, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1208, 'Loss', 0.31027284374290076, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1209, 'Loss', 0.31004654427364964, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1210, 'Loss', 0.30982058917262734, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1211, 'Loss', 0.30959497764487126, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1212, 'Loss', 0.30936970889787252, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1213, 'Loss', 0.30914478214157637, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1214, 'Loss', 0.30892019658836395, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1215, 'Loss', 0.3086959514530439, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1216, 'Loss', 0.30847204595284949, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1217, 'Loss', 0.30824847930742144, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1218, 'Loss', 0.30802525073879999, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1219, 'Loss', 0.30780235947142287, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1220, 'Loss', 0.30757980473210766, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1221, 'Loss', 0.30735758575004596, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1222, 'Loss', 0.30713570175679455, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1223, 'Loss', 0.30691415198626393, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1224, 'Loss', 0.30669293567471589, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1225, 'Loss', 0.30647205206074091, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1226, 'Loss', 0.30625150038526944, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1227, 'Loss', 0.30603127989154039, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1228, 'Loss', 0.30581138982511091, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1229, 'Loss', 0.30559182943383795, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1230, 'Loss', 0.30537259796787281, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1231, 'Loss', 0.30515369467964476, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1232, 'Loss', 0.30493511882386992, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1233, 'Loss', 0.30471686965751865, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1234, 'Loss', 0.30449894643983061, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1235, 'Loss', 0.30428134843228966, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1236, 'Loss', 0.30406407489862119, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1237, 'Loss', 0.30384712510478312, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1238, 'Loss', 0.30363049831895894, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1239, 'Loss', 0.30341419381155021, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1240, 'Loss', 0.30319821085515763, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1241, 'Loss', 0.30298254872459074, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1242, 'Loss', 0.30276720669683999, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1243, 'Loss', 0.30255218405108603, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1244, 'Loss', 0.30233748006868172, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1245, 'Loss', 0.30212309403314308, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1246, 'Loss', 0.30190902523014512, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1247, 'Loss', 0.30169527294751475, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1248, 'Loss', 0.3014818364752167, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1249, 'Loss', 0.3012687151053528, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1250, 'Loss', 0.30105590813214528, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1251, 'Loss', 0.30084341485194005, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1252, 'Loss', 0.30063123456318791, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1253, 'Loss', 0.30041936656644097, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1254, 'Loss', 0.30020781016434589, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1255, 'Loss', 0.29999656466163932, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1256, 'Loss', 0.2997856293651282, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1257, 'Loss', 0.29957500358369615, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1258, 'Loss', 0.29936468662828486, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1259, 'Loss', 0.29915467781189325, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1260, 'Loss', 0.29894497644956436, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1261, 'Loss', 0.29873558185838739, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1262, 'Loss', 0.29852649335747494, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1263, 'Loss', 0.29831771026796766, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1264, 'Loss', 0.2981092319130243, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1265, 'Loss', 0.29790105761781099, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1266, 'Loss', 0.29769318670949479, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1267, 'Loss', 0.29748561851723887, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1268, 'Loss', 0.2972783523721923, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1269, 'Loss', 0.29707138760748386, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1270, 'Loss', 0.29686472355821503, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1271, 'Loss', 0.29665835956145048, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1272, 'Loss', 0.29645229495621322, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1273, 'Loss', 0.29624652908347876, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1274, 'Loss', 0.2960410612861622, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1275, 'Loss', 0.29583589090911538, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1276, 'Loss', 0.29563101729912378, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1277, 'Loss', 0.29542643980488548, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1278, 'Loss', 0.29522215777702226, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1279, 'Loss', 0.2950181705680584, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1280, 'Loss', 0.29481447753241896, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1281, 'Loss', 0.29461107802642483, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1282, 'Loss', 0.2944079714082784, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1283, 'Loss', 0.29420515703807071, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1284, 'Loss', 0.29400263427775591, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1285, 'Loss', 0.29380040249116013, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1286, 'Loss', 0.29359846104396714, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1287, 'Loss', 0.29339680930371681, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1288, 'Loss', 0.29319544663978042, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1289, 'Loss', 0.29299437242338572, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1290, 'Loss', 0.29279358602758115, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1291, 'Loss', 0.29259308682724711, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1292, 'Loss', 0.29239287419907678, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1293, 'Loss', 0.29219294752157993, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1294, 'Loss', 0.29199330617506963, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1295, 'Loss', 0.29179394954165805, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1296, 'Loss', 0.29159487700524916, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1297, 'Loss', 0.29139608795153499, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1298, 'Loss', 0.29119758176798533, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1299, 'Loss', 0.2909993578438419, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1300, 'Loss', 0.2908014155701138, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1301, 'Loss', 0.29060375433957314, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1302, 'Loss', 0.29040637354673982, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1303, 'Loss', 0.29020927258788776, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1304, 'Loss', 0.29001245086102362, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1305, 'Loss', 0.28981590776589772, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1306, 'Loss', 0.28961964270398294, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1307, 'Loss', 0.28942365507847689, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1308, 'Loss', 0.28922794429428988, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1309, 'Loss', 0.28903250975804856, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1310, 'Loss', 0.28883735087807644, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1311, 'Loss', 0.28864246706439922, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1312, 'Loss', 0.28844785772873327, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1313, 'Loss', 0.28825352228447637, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1314, 'Loss', 0.28805946014671102, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1315, 'Loss', 0.28786567073219171, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1316, 'Loss', 0.28767215345933556, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1317, 'Loss', 0.28747890774822954, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1318, 'Loss', 0.28728593302060879, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1319, 'Loss', 0.28709322869985932, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1320, 'Loss', 0.286900794211014, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1321, 'Loss', 0.28670862898074195, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1322, 'Loss', 0.28651673243734077, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1323, 'Loss', 0.28632510401073924, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1324, 'Loss', 0.2861337431324833, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1325, 'Loss', 0.2859426492357337, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1326, 'Loss', 0.28575182175526143, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1327, 'Loss', 0.28556126012744115, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1328, 'Loss', 0.28537096379023791, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1329, 'Loss', 0.28518093218321688, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1330, 'Loss', 0.28499116474752778, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1331, 'Loss', 0.28480166092589748, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1332, 'Loss', 0.284612420162628, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1333, 'Loss', 0.28442344190359298, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1334, 'Loss', 0.28423472559622975, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1335, 'Loss', 0.28404627068953126, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1336, 'Loss', 0.28385807663404483, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1337, 'Loss', 0.28367014288186349, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1338, 'Loss', 0.28348246888662587, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1339, 'Loss', 0.28329505410350198, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1340, 'Loss', 0.28310789798919428, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1341, 'Loss', 0.28292100000193232, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1342, 'Loss', 0.28273435960146431, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1343, 'Loss', 0.28254797624905575, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1344, 'Loss', 0.28236184940747522, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1345, 'Loss', 0.28217597854100185, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1346, 'Loss', 0.28199036311541209, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1347, 'Loss', 0.28180500259797481, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1348, 'Loss', 0.28161989645744756, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1349, 'Loss', 0.28143504416407078, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1350, 'Loss', 0.28125044518956388, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1351, 'Loss', 0.28106609900711887, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1352, 'Loss', 0.28088200509139571, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1353, 'Loss', 0.28069816291851507, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1354, 'Loss', 0.280514571966057, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1355, 'Loss', 0.28033123171305391, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1356, 'Loss', 0.28014814163998364, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1357, 'Loss', 0.27996530122876762, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1358, 'Loss', 0.27978270996276661, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1359, 'Loss', 0.27960036732676941, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1360, 'Loss', 0.27941827280699488, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1361, 'Loss', 0.27923642589108466, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1362, 'Loss', 0.27905482606809451, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1363, 'Loss', 0.27887347282849673, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1364, 'Loss', 0.27869236566416927, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1365, 'Loss', 0.27851150406839081, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1366, 'Loss', 0.27833088753584251, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1367, 'Loss', 0.27815051556259685, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1368, 'Loss', 0.27797038764610971, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1369, 'Loss', 0.27779050328522931, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1370, 'Loss', 0.2776108619801741, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1371, 'Loss', 0.2774314632325402, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1372, 'Loss', 0.27725230654529603, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1373, 'Loss', 0.27707339142276971, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1374, 'Loss', 0.27689471737064786, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1375, 'Loss', 0.27671628389597985, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1376, 'Loss', 0.27653809050715722, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1377, 'Loss', 0.27636013671392284, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1378, 'Loss', 0.27618242202735627, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1379, 'Loss', 0.27600494595987873, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1380, 'Loss', 0.27582770802523943, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1381, 'Loss', 0.27565070773851852, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1382, 'Loss', 0.27547394461611435, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1383, 'Loss', 0.27529741817574804, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1384, 'Loss', 0.27512112793645338, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1385, 'Loss', 0.27494507341857277, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1386, 'Loss', 0.27476925414375236, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1387, 'Loss', 0.27459366963494281, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1388, 'Loss', 0.27441831941638783, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1389, 'Loss', 0.27424320301362276, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1390, 'Loss', 0.27406831995347342, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1391, 'Loss', 0.27389366976404278, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1392, 'Loss', 0.27371925197471714, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1393, 'Loss', 0.27354506611615492, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1394, 'Loss', 0.2733711117202875, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1395, 'Loss', 0.27319738832030466, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1396, 'Loss', 0.2730238954506688, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1397, 'Loss', 0.27285063264708559, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1398, 'Loss', 0.2726775994465232, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1399, 'Loss', 0.27250479538719879, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1400, 'Loss', 0.27233222000856572, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1401, 'Loss', 0.27215987285132609, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1402, 'Loss', 0.27198775345741361, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1403, 'Loss', 0.27181586136999375, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1404, 'Loss', 0.27164419613345808, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1405, 'Loss', 0.2714727572934294, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1406, 'Loss', 0.27130154439673948, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1407, 'Loss', 0.27113055699144278, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1408, 'Loss', 0.2709597946268022, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1409, 'Loss', 0.27078925685328681, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1410, 'Loss', 0.27061894322257057, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1411, 'Loss', 0.27044885328752599, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1412, 'Loss', 0.27027898660221861, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1413, 'Loss', 0.27010934272190995, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1414, 'Loss', 0.26993992120304272, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1415, 'Loss', 0.26977072160324572, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1416, 'Loss', 0.26960174348132765, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1417, 'Loss', 0.26943298639727026, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1418, 'Loss', 0.26926444991222642, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1419, 'Loss', 0.26909613358852053, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1420, 'Loss', 0.26892803698963336, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1421, 'Loss', 0.26876015968021305, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1422, 'Loss', 0.26859250122605716, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1423, 'Loss', 0.26842506119411874, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1424, 'Loss', 0.2682578391524959, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1425, 'Loss', 0.26809083467043282, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1426, 'Loss', 0.26792404731831782, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1427, 'Loss', 0.26775747666766836, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1428, 'Loss', 0.26759112229113946, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1429, 'Loss', 0.26742498376251439, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1430, 'Loss', 0.26725906065670235, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1431, 'Loss', 0.2670933525497311, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1432, 'Loss', 0.26692785901875415, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1433, 'Loss', 0.26676257964202793, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1434, 'Loss', 0.26659751399892911, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1435, 'Loss', 0.26643266166993745, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1436, 'Loss', 0.26626802223663437, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1437, 'Loss', 0.2661035952817058, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1438, 'Loss', 0.26593938038892878, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1439, 'Loss', 0.26577537714317312, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1440, 'Loss', 0.26561158513040162, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1441, 'Loss', 0.26544800393765772, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1442, 'Loss', 0.26528463315306988, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1443, 'Loss', 0.26512147236584427, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1444, 'Loss', 0.26495852116625768, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1445, 'Loss', 0.26479577914566149, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1446, 'Loss', 0.26463324589647597, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1447, 'Loss', 0.26447092101218389, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1448, 'Loss', 0.26430880408732604, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1449, 'Loss', 0.26414689471750402, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1450, 'Loss', 0.26398519249936964, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1451, 'Loss', 0.26382369703062963, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1452, 'Loss', 0.26366240791003581, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1453, 'Loss', 0.26350132473737753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1454, 'Loss', 0.26334044711349303, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1455, 'Loss', 0.26317977464025227, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1456, 'Loss', 0.26301930692055803, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1457, 'Loss', 0.26285904355834583, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1458, 'Loss', 0.26269898415857579, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1459, 'Loss', 0.26253912832723075, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1460, 'Loss', 0.2623794756713157, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1461, 'Loss', 0.26222002579884857, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1462, 'Loss', 0.26206077831886376, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1463, 'Loss', 0.26190173284140678, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1464, 'Loss', 0.2617428889775234, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1465, 'Loss', 0.26158424633926936, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1466, 'Loss', 0.26142580453969905, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1467, 'Loss', 0.26126756319286271, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1468, 'Loss', 0.26110952191380327, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1469, 'Loss', 0.26095168031855753, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1470, 'Loss', 0.26079403802414691, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1471, 'Loss', 0.26063659464857802, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1472, 'Loss', 0.26047934981083887, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1473, 'Loss', 0.26032230313089466, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1474, 'Loss', 0.26016545422968507, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1475, 'Loss', 0.26000880272912313, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1476, 'Loss', 0.25985234825208836, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1477, 'Loss', 0.25969609042242447, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1478, 'Loss', 0.25954002886494076, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1479, 'Loss', 0.25938416320540558, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1480, 'Loss', 0.25922849307054174, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1481, 'Loss', 0.25907301808802363, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1482, 'Loss', 0.25891773788647676, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1483, 'Loss', 0.25876265209547572, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1484, 'Loss', 0.25860776034553834, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1485, 'Loss', 0.25845306226812004, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1486, 'Loss', 0.25829855749561947, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1487, 'Loss', 0.25814424566136446, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1488, 'Loss', 0.2579901263996201, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1489, 'Loss', 0.25783619934557456, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1490, 'Loss', 0.25768246413535106, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1491, 'Loss', 0.2575289204059848, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1492, 'Loss', 0.25737556779543991, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1493, 'Loss', 0.25722240594259493, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1494, 'Loss', 0.25706943448724068, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1495, 'Loss', 0.25691665307007949, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1496, 'Loss', 0.25676406133272628, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1497, 'Loss', 0.2566116589176976, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1498, 'Loss', 0.25645944546841515, 'Training Accuracy', 100.0)\n",
      "('Epoch', 1499, 'Loss', 0.2563074206291997, 'Training Accuracy', 100.0)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "w = np.random.randn(1,3)\n",
    "for i in range(1,1500):\n",
    "    \n",
    "    z = np.dot(w,Data)\n",
    "    y_pred = prediction(w, Data)\n",
    "    val = -np.multiply(y,z)\n",
    "    J = np.sum(np.log(1+np.exp(val)))\n",
    "    num = -np.multiply(y,np.exp(val))\n",
    "    den = 1+np.exp(val)\n",
    "    f = num/den\n",
    "    gradJ = np.dot(Data,f.T)\n",
    "    w = w - learning_rate*gradJ.T\n",
    "\n",
    "    print(\"Epoch\",i,\"Loss\",J,\"Training Accuracy\",accuracy_score(y[0], y_pred)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test Accuracy', 95.0)\n"
     ]
    }
   ],
   "source": [
    "Test_predict = prediction(w, v_Data)\n",
    "print(\"Test Accuracy\",accuracy_score(v_y[0], Test_predict)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd4VGX6xvHvkwIYejcghCqINCVS\npMRCtQD2gr2ACCrgruDqrt2fZTcUBVysCNhBQUQpFkITDUgXBZQqvWPovL8/EtyIKZNkZs5M5v5c\n17mSKTnvM1x6z5n3vOcZc84hIiKFX5TXBYiISHAo8EVEIoQCX0QkQijwRUQihAJfRCRCKPBFRCKE\nAl9EJEIo8EVEIoQCX0QkQsR4XUBmFSpUcDVq1PC6DBGRsLJgwYIdzrmKuT0vpAK/Ro0apKamel2G\niEhYMbN1vjxPUzoiIhFCgS8iEiEU+CIiEUKBLyISIRT4IiIRQoEvIhIhFPgiIhGiUAT+zgOHefLT\nFew7dNTrUkREQlaBA9/MqpnZ12a2wsyWm9kDGfeXM7PpZrYq42fZgpebtTlrdvLW3F/pkDyTGSu2\nBmoYEZGw5o8j/GPAg865BkBLoI+ZNQAGAV865+oCX2bcDoiuTarw8b2tKRtXhLveTuX+d39g54HD\ngRpORCQsFTjwnXObnXMLM37fD/wIVAW6AaMznjYa6F7QsXLSpFoZJvVtw4AOZ/L5ss20T57JxEWb\ncM4FclgRkbDh1zl8M6sBnAPMByo75zZnPLQFqJzN3/Q0s1QzS92+fXuBxi8SE8X9F9fls/vbklC+\nOA+8t4i7Rqeyee/BAu1XRKQw8Fvgm1kJYDzQzzm3L/NjLv0wO8tDbefcKOdconMusWLFXJu9+eTM\nyiUZ3/t8Hr30LOau2UmH5BTGzV/HiRM62heRyOWXwDezWNLDfpxzbkLG3VvNLD7j8Xhgmz/G8lV0\nlHFX21pM7deOJtVK88jHy7jh1W/5dcfvwSxDRCRk+GOVjgGvAz8655IzPTQJuDXj91uBiQUdKz+q\nl49j7J0teP6qRqzYvI/OQ1IYlbKGY8dPeFGOiIhnrKAnNc2sDTALWAqcTNF/kD6P/wFQHVgHXOuc\n25XTvhITE10g++Fv3XeIRz9ZxvQVW2l8Rmmev6oxZ8WXCth4IiLBYGYLnHOJuT4vlFaxBDrwAZxz\nTFm6hccmLWNP2lHuvaA2fS6qQ9GY6ICOKyISKL4GfqG40jYvzIxLG8czvX8SXZtUYdhXq7l02GwW\nrt/tdWkiIgEVcYF/UtniRUi+rilv3n4eaYePcdXIuTz56QrSjhzzujQRkYCI2MA/6cJ6lZjavx03\ntUjgjTm/0mlICrNX7fC6LBERv4v4wAcoWSyWp7o35INerYiNiuKm1+fz0EeL2XtQzdhEpPBQ4GfS\nvGY5pjzQlt4X1Gb8wk10SJ7J1OVbvC5LRMQvFPinKBYbzcDO9ZnYpzXlSxSl15gF9Bm3kO371YxN\nRMKbAj8bDauWZlLf1vy9Uz2mr9hKh8EzmbBwo5qxiUjYUuDnIDY6ij4X1mHKA22oVaE4Az5YzO1v\nfc+mPWrGJiLhR4HvgzqVSvLhPefz+OUN+O7XXXRMnsmYeWvVjE1EwooC30fRUcZtrWsytV87zk0o\nyz8nLuf6Ud/yy/YDXpcmIuITBX4eVSsXx9t3NOfFqxuzcss+Og+dxchv1IxNREKfAj8fzIxrEqsx\nY0ASF9WrxPNfrKT7iDks/22v16WJiGRLgV8AlUoV45WbmzGyx7ls2XuYri/P4cWpKzl09LjXpYmI\n/IUC3w+6NIpnxoB2dG9aleFfr+HSYbNYsC7HTtAiIkGnwPeTMnFF+M+1TRh9R3MOHT3B1a/M4/FJ\ny/n9sJqxiUhoUOD7WdKZFZnavx23tExg9Ly1dBycQsrPBftydhERf1DgB0CJojE80a0hH/ZqRdHY\nKG554zv+9uFi9qQd8bo0EYlgCvwASqxRjin3t6XPhbX5+IdNtE9O4Ytlm70uS0QilAI/wIrFRvP3\nTvWZ1Lc1lUsV5Z6xC+k9dgHb9h/yujQRiTAK/CA5u0ppPunTmoc61+PLldvokJzCRwvUjE1EgkeB\nH0Sx0VHce0EdptzflrqVSvC3Dxdz65vfs3F3mteliUgEUOB7oE6lEnzQqxVPdjubBWt30XFwCqPn\nqhmbiASWXwLfzN4ws21mtizTfY+b2SYzW5SxXeKPsQqLqCjjllY1mNq/HYk1yvHYpOVc+995rN6m\nZmwiEhj+OsJ/C+icxf2DnXNNM7YpfhqrUDmjbByjbz+P/1zThFXbDnDJ0FkM/3o1R9WMTUT8zC+B\n75xLAdRLIJ/MjKuancGMAUl0aFCZF6f+RLeX57Bsk5qxiYj/BHoOv6+ZLcmY8ikb4LHCXsWSRRne\n41xeuakZ2w8cptvwOTz/hZqxiYh/BDLwRwK1gabAZuA/WT3JzHqaWaqZpW7frhYEAJ0bns6M/klc\ndW5VRn6zhkuGzuL7tfoAJSIFE7DAd85tdc4dd86dAF4FmmfzvFHOuUTnXGLFihUDVU7YKR0XywtX\nN2HsnS04cvwE17wyj39NXMYBNWMTkXwKWOCbWXymm1cAy7J7rmSvTd0KTO3Xjjta12TMt+vomDyT\nb37a5nVZIhKG/LUs811gHlDPzDaa2Z3AC2a21MyWABcC/f0xViQqXjSGf13egI/uOZ+4ojHc9ub3\nDHh/Ebt/VzM2EfGdhdKl/YmJiS41NdXrMkLa4WPHGf7VakZ8s4YycbE80bUhlzQ6HTPzujQR8YiZ\nLXDOJeb2PF1pG2aKxkQzoGM9Pr2vDfGlT6PPOwvpNWYB2/apGZuI5EyBH6bOii/Fx/eez8Nd6jPz\n5+20T57JB99vUDM2EcmWAj+MxURH0SupNp8/0Jb68aV4aPwSbn79OzbsUjM2EfkrBX4hUKtiCd67\nuyVPd2/Iog176Dg4hddn/8pxNWOTbIwbBzVqQFRU+s9x47yuSIJBgV9IREUZN7VMYFr/drSsVY6n\nJq/g6lfmsmrrfq9Lk1N4HbbjxkHPnrBuHTiX/rNnT4V+JFDgFzJVypzGG7edx5DrmrJ2x+9cOmw2\nw75cxZFjasaWF4EK5dzCNi/j5rfGRx6BtFNm/dLS0u+XQs45FzJbs2bNnPjP9v2HXJ9xC1zCwMmu\n0+CZbvGG3V6XFBbGjnUuLs659EhO3+Li0u8vqISEP+/35JaQkPW4Zn9+PKcazZzr3Tv3Gk7u89TN\nrOCvT7wBpDofMlbr8CPA9BVbefSTpWzff5i729aiX/szOa1ItNdlhawaNdKPvE+VkABr1xZs31FR\n6fF6KjOoXj3rcU+Ki4NRo6BHj+xrNIMxY9Kfk51Avj7xhtbhyx86NKjMtP5JXJtYjf+m/EKXoSl8\n+8tOr8sKKZmnR7IL3fXrCz5O9erZ35/b/tPS4NZbc67RudynZp55Jv3NI7O4uPT7pXBT4EeI0qfF\n8txVjXnnrhaccHD9qG/5x8dL2X/oqNelee7UefXsZBfW2e0zq/n1nMLWl/0fP55zjZD7G0ePHumf\nFBIS0j8RJCT875ODFHK+zPsEa9McfnCkHT7mnvp0uas5aLJr+ewM9+WPW/K8j7Fj0+eVzf46v+xP\nwRgnu3n1U+e3fR07t3MA2b2msWNzr8OXLSHB//9GEtrwcQ7f85DPvCnwg+uH9btdx+SZLmHgZHf/\nuwvdzgOHffq7QJ7U9GKc7E5inrr5KqcTsznxR+AH4t9HQp+vga+TthHuyLETDP96NSO+WU3JYrE8\n3vVsLm8cn2MztmCd9PN6nPyOmdOJ2RM5rI71pY5TlS8PJUqkT+NUr54+NaSpmcijk7bikyIxUfTv\ncCaT72tLtXJx3P/uD9z99gK27M2+GVt2c8T+OKnpxTjPPAOxsdk/Hhvr+wnNcePSAz8ruc3R5/V1\nxcXB0KHpb0QnTqT/VNhLThT4AkC900syoff5PHrpWcxevZ0OyTN597v1ZPUJMKeVJv4UrHEg/eg7\nO6VK+RakJ0/+Hs/iK4jN0o/ec7pAKi+vSydaJT8U+PKH6Cjjrra1+OKBdpxdtRQPT1jKja/OZ93O\n3//0vGAt6wvWOI88Akdy+C6ZXT5+nXBWV7CedPJ9M6c2Blm93qyY6Whe8keBL39Ro0Jx3rmrJf93\nZSOWbdpLpyEpvDbrlz+asQVrWV9u4/ir/UFuUyk5HXn7sn7/VNm1MTj19UZnc21cID7hSITw5cxu\nsDat0gk9m/ccdHe+9Z1LGDjZdX15tlu5eZ/XJTnn/LuCJ6dlmTntM6safN18aWMQrFVKEv7wcZWO\njvAlR6eXLsartyQy7IZz2LArjctemsWQGT973ozNnw3ALrkk6/uLF8/5k0tOUzi58eUoXRdIib8p\n8CVXZkbXJlWYMSCJSxvFM2TGKi5/aTaLNuzxrCZ/ruCZMiXr+ytUyDlc87taKLvzEFlNUfXokbdV\nOF63XpbQpsAXn5UrXoQh15/DG7clsu/QUa4cMYenJ6/g4JEslqUEWEFX8Pijd46vY508Os/pKN0f\nPerV515y5cu8T7A2zeGHj30Hj7h/TFjiEgZOdm2f/8rNWbU9qOMXZH7b17l3X66M9WU/vrQszu/V\nuf7eh4QnNIcvgVSyWCzPXNGI93q2JMrgxtfm8/CEJewLUjO2gsxv+zL37svyz6xquPji/62uiY6G\n3r1hxIjca/LHFFWwLlST8OWX1gpm9gZwGbDNOdcw475ywPtADWAtcK1zbndO+1FrhfB08Mhxhsz4\nmVdn/ULFkkV5unsjOjSo7HVZ2cqu9QH8ry99sFsU+KONhPrcR65gt1Z4C+h8yn2DgC+dc3WBLzNu\nSyF0WpFoHr7kLD7p05qycUW4++1U+r6zkB0HDntdWpaym3tPSPDu5Kg/LjJTn3vJlS/zPr5spB/J\nL8t0+ycgPuP3eOCn3PahOfzwd/jocTdsxs+u7j+muKZPTHUfL9zoTpw44XVZf1LQ9e2BWh/vj1bQ\nwWpbLaGFYHfLNLMawGT3vymdPc65Mhm/G7D75O3saEqn8Fi1dT8PjV/CD+v3cFH9SjzdvSFVypzm\ndVl/GDcufS4/P10mNXUiocbXKZ2gBH7G7d3OubJZ/F1PoCdA9erVm63La39YCVnHTzhGz13Li1N/\nIjrKGNSlPjc2r05UVA6dysJAftsfiwRKKLRH3mpm8RnFxAPbsnqSc26Ucy7ROZdYsWLFAJYjwRYd\nZdzRpiZT+7WjSbXSPPrJMm549Vt+3fF77n8cwoLZxVPEnwIZ+JOAWzN+vxWYGMCxJIRVLx/H2Dtb\n8PxVjVixeR+dh6Tw35lrOHY8PA+HA3FyVFfISjD4JfDN7F1gHlDPzDaa2Z3Ac0AHM1sFtM+4LRHK\nzLjuvOrMGJBEuzMr8n+fr+TKkXP5cfM+r0vLM3/3uNEVshIs+opDCTrnHFOWbuGxScvYk3aUey+o\nTZ+L6lA0Jpt+wIWcTgJLQYXCHL5IlsyMSxvHM71/El2bVGHYV6u5bNhsFq7P8bq8QktXyEqwKPDF\nM2WLFyH5uqa8eft5/H74GFeNnMuTn64g7cgxr0sLKp0ElmBR4IvnLqxXiWkDkripRQJvzPmVjoNT\nmL1qh9dlBY2ukJVgUeBLSChRNIanujfkg16tiI2O4qbX5/PQR4vZezA4zdi8pC86kWDRSVsJOYeO\nHmfIjFW8OusXyhcvwlPdG9Lp7NO9LkskZOmkrYStYrHRDOpSn4l9WlOhRFF6jVlAn3EL2b4/NJux\niYQLBb6ErIZVSzOxb2v+3qke01dspX3yTMYv2EgofSoVCScKfAlpsdFR9LmwDlMeaEudSiV48MPF\n3Pbm92zac9Dr0kTCjgJfwkKdSiX4oFcrHru8Ad+v3UXH5JmMmbeWEyd0tC/iKwW+hI3oKOP21unN\n2M5NKMs/Jy7n+lHf8sv2A16XJhIWFPiFSYR04KpWLo6372jOi1c3ZuWWfXQeOouR34RvMzaRYFHg\nFxYR1oHLzLgmsRozHkzionqVeP6LlXQfMYflv+31ujSRkKV1+IVFhHfg+nzpZv45cTm7045wT1It\n7ruoLsViI7MZm0QercOPNBHegatLo3hmDGhH96ZVGf71Gi4dNosF63Z5XZZISFHgFxbqwEWZuCL8\n59omjL6jOYeOnuDqV+bx+KTl/H44spqxiWRHgV9YqAPXH5LOrMi0/u24tVUNRs9bS8fBKaT8vN2/\ng0TICXIpXBT4hYU6cP1J8aIxPN71bD7s1YqisVHc8sZ3/O3DxexJO1LwnUfYCXIpPHTSVgq9Q0eP\n8/JXqxk5cw1l44rwVLez6dIoPj2gH3kk/TxH9erpn4Z8eYOM8BPkEnp8PWmrwJeIsfy3vQwcv4Rl\nm/bRudQRnhzcl0o7fvvfE+LifPtUFBWVfmR/KjM4oWsBJPi0SkfkFGdXKc0n97ZmYOf6fLXbaN8j\nmQ8ateeP6E5LSz/iz41OkEuYUuBLRImJjqL3BbX5/M37qLd9HQ9d0o9brn2SDaUqpT/Bl2WsOkEu\nYUqBH2mCvbokRFez1C4RzfvvDOKpaSNYWKU+ne4czlvnXsaJ6gm5/7FOkEu4cs4FdAPWAkuBRUBq\nTs9t1qyZkwAaO9a5uDjn0meg07e4uPT7vRxv7FjnEhKcM0v/Gah6sqltQ6mK7uZrnnAJAye7K5/4\nxK3aui/w44v4UW7ZenIL+ElbM1sLJDrncv1Wap20DbBgry7xZbyTSxzT0v73uK8nTwsq0yodV706\nE/72Ak/tKkPa4ePcf3EdeiXVJjZaH4Il9IXMKh0FfggJ9uoSX8YLsSWO2/cf5vFJy/ls6WbOii/F\ni1c3pmHV0kGvQyQvQmmVjgOmmdkCM+sZhPHkpFPnz8uVy/p5gVpd4stqFn/1APLTuYKKJYsyvMe5\nvHJTM3YcOEy34XN47vOVHDp6PF/7Ewkpvsz7FGQDqmb8rAQsBtqd8nhPIBVIrV69eoBmuCJQVvPn\nsbHOFSmS9zn8/M6x+zKHn5Dw58dPbgkJvteR1TjgXPnyBTofsOf3I+7vHy5yCQMnuwtf/NrN/2Vn\nvvclEkj4OIcf8MD/02DwOPC37B7XSVs/yi5Iy5fPW3gX9ERvbm8WeTmxm93zsnutfjopPevn7a7N\n81+6hIGT3aMfL3X7Dh4p0P5E/C0kAh8oDpTM9PtcoHN2z1fg+5FZ1gFolrf95BSm/lpRk/lNoXz5\n9O3UN4icPglk91rz82khG78fPuqemLTc1Rg02bV6dob7auXWgr9uET8JlcCvlTGNsxhYDjyS0/MV\n+H6Ul6mSnOQWpv5c1pnTUXxOb2A5vSll9SaX1ThmzvXunWuJC9btcu3/841LGDjZ9XvvB7frwGH/\nvHaRAgiJwM/rpsD3I3+tuc8tTPPyJpLTHHxunyRyerx48bzVl9O+fAj9Q0ePuf9MXelqP/yZO/fJ\nae7TxZvciRMnfPs3EAkABX5hk58Tp/64oCm7E6J5nSbK7g2od2/f9u9LHb5+AsnpU8vJsXyw4re9\n7vKXZrmEgZPdXaO/d1v2Hsz7v6+IHyjwC5NgXyGb1fi5HYHnJru/j47OPbRP7n/s2PT5/bwEflZH\n7Ll9asnDtNfRY8fdK9+sdmc+MsU1fOwL995363S0L0Hna+DrMsJw8Mgjf74SFXzv7JhZfteq9+iR\nfhHU2LF/bRoWGwsHDqTvs0KF9C2r/We3rv54LuvbT21Ktn+/bzWfNHr0X1/nM8+kX/yVnTxcAxAT\nHUWvpNp80a8dZ8WXYuD4pdz0+nzW70zL/Y9Fgs2Xd4VgbTrCz4Y/Vtz461PCqStqTl3Xn93+83OE\nHx3t25p9Xz8hZNa7t1+O8DM7fvyEGzNvrTv7X1+4+o9+7l6b9Ys7dlxH+xJ4aEqnEPHHiht/rdrx\nZZ/ZTcdkN4ef03x6ZrmtGPJ1Pyf17v3XffphqmzT7jR329Mfu4SBk133m/7tfm7UInjTbxKRfA18\nTemEA3/0X/dXC4O8/u3J52TXUnjECChfPuu/PbU1Q04tIMqXT9+nL/s5acQIGDPG722Oq3w2gTee\n7cHQSS+ytmw8l3YcxLBXp3JkTGi0hpYI5su7QrA2HeHnoKArbrw+ws9JXq62zWoKKTY2/TGvT26f\nlOnfZcdppVzfy//uEgZOdp16/dctWr87uLVIREBTOvIngQjD3JZK+rMFQ+bnZV6pc2q/HC96658q\ni6mnaXWau+b3jnY1B012z362wqUdPhb8uqTQUuDLXwUiDH1pixBpsvnks7dOPTdo/GKXMHCyS3rh\nKzdvzY4//10ovFlJWPI18APeDz8v1A9fCoVcvtRl7uodDJqwlPW70rixRXUGdalPqfEfePdFMBL2\nQqkfvkhkyeU7b8+vU4Gp/dpxd9uavPfdejomp/DVy+/451oLkRzoCF/EQ4s27GHgR0v4aet+ui3/\nhn99OYryB/f97wmB+jYyKVR0hC8SBppWK8On97Wh39LPmFK/NR3uGsnEs9rxx2FYoL6NTCKSAl/E\nY0Viouh3Y2smvzeIanu28EDXh7j7yn+ypWLVvF1rIZKLGK8LEBGgRw/qARMefZQ3Kzbl3+1uoUO9\nV/hH3cZc7xyWU+8fER9pDl8kBK3b+TsPT1jK3DU7aVWrPM9d1YiE8sW9LktClObwRcJYQvnijLur\nBc9d2Yhlm/bSaUgKr836heMnQucATcKPAl8kRJkZ1zevzvQBSbSpU4GnP/uRK0fO5acteWwRLZJB\ngS8S4k4vXYxXb0nkpRvOYeOuNC57aRaDp//MkWNaril5o8AXCQNmxuVNqjB9QBKXNIpn6JeruOyl\nWSzasMfr0iSMKPBFwki54kUYev05vHFbIvsPHePKEXN4evIKDh7J5ZvDRFDgi4Sli+pXZlr/dtzQ\nvDqvzf6VTkNSmLtmh9dlSYhT4IuEqZLFYnnmika817MlUQY3vjqfhycsYe/Bo16XJiEq4IFvZp3N\n7CczW21mgwI9nkikaVmrPF/0a0evpFq8//0GOg6eyfQVW70uS0JQQAPfzKKB4UAXoAFwg5k1COSY\nIpGoWGw0D3c5i0/6tKZsXBHufjuVvu8sZMeBw16XJiEk0Ef4zYHVzrlfnHNHgPeAbgEeUyRiNT6j\nDJP6tmFAhzOZtnwrHZJn8skPmwilK+rFO4EO/KrAhky3N2bcJyIBUiQmivsvrstn97ehRoXi9Ht/\nEXe89T2/7TnodWniMc9P2ppZTzNLNbPU7du3e12OSKFRt3JJPrrnfP51WQO+/WUXHQenMObbdZxQ\ne4aIFejA3wRUy3T7jIz7/uCcG+WcS3TOJVasWDHA5YhElugo4442NZnWvx1NqpXmn58s4/pXv+WX\n7Qe8Lk08EOjA/x6oa2Y1zawIcD0wKcBjisgpqpWLY+ydLXj+qkb8uHkfXYbO4pWZazh2XO0ZIklA\nA985dwzoC0wFfgQ+cM4tD+SYIpI1M+O686ozY0ASSWdW5LnPV3LFiLms+G1f7n8shYL64YtEIOcc\nU5Zu4bFJy9iTdpTeF9Sm70V1KBoT7XVpkg/qhy8i2TIzLm0cz/T+SXRtUoWXvlrNpcNms2Ddbq9L\nkwBS4ItEsLLFi5B8XVPeuv080g4f4+pX5vLEp8tJO3LM69IkABT4IsIF9SoxbUASN7dM4M05a+k4\nOIXZq9SMrbBR4IsIACWKxvBkt4Z8eE8rikRHcdPr83noo8XsTVMztsJCgS8if3JejXJMeaAt915Q\nm/ELN9F+8Ey+WLbF67LEDxT4IvIXxWKjeahzfSb2aU3FEkW5Z+wC7h23gG37D3ldmhSAAl9EstWw\namkm9m3N3zvVY8aP2+iQnML4BRvVjC1MKfBFJEex0VH0ubAOU+5vS51KJXjww8Xc+ub3bNyd5nVp\nkkcKfBHxSZ1KJfiwVyue6Ho2qWvTm7GNnrtWzdjCiAJfRHwWFWXcen4NpvVvR7OEsjw2aTnXjZrH\nGjVjCwsKfBHJszPKxvH2Hc158erG/LRlP12GzmLEN6s5qmZsIU2BLyL5YmZck1iNGQ8mcVG9Srzw\nxU90Hz6HZZv2el2aZEOBLyIFUqlkMV65uRkje5zL1n2H6TZ8Di98sZJDR497XZqcQoEvIn7RpVE8\nMwa044pzqjLimzVcMmwWqWt3eV2WZKLAFxG/KRNXhH9f04S372jO4aMnuOa/83hs4jIOHFYztlCg\nwBcRv2t3ZkWm9W/Hra1q8Pa36+g0OIWZP+s7q72mwBeRgCheNIbHu57NR/e0olhsFLe+8R0PfrCY\nPWlHvC4tYinwRSSgmiWU47P729L3wjpMXLSJ9skpfL50s9dlRSQFvogEXLHYaP7WqR4T+7bm9NJF\n6T1uIfeMWcC2fWrGFkwKfBEJmrOrlOaTe1szsHN9vvppG+2TZ/Jh6gY1YwsSBb6IBFVMdBS9L6jN\n5w+0pd7pJfn7R0u45Y3v2LBLzdgCTYEvIp6oXbEE7/dsxZPdzmbhut10GpLCm3N+5biasQVMwALf\nzB43s01mtihjuyRQY4lIeIqKMm5pVYNpA5I4r0Y5nvh0Bdf+dx6rt+33urRCKdBH+IOdc00ztikB\nHktEwlTVMqfx1u3nkXxtE9ZsP8AlQ2fz8ler1IzNzzSlIyIhwcy48twzmN4/iQ5nV+bf036m68tq\nxuZPgQ78vma2xMzeMLOyAR5LRAqBiiWLMvzGc/nvzc3YeSC9Gdtzn6sZmz9YQZZDmdkM4PQsHnoE\n+BbYATjgKSDeOXdHFvvoCfQEqF69erN169blux4RKVz2HjzKs5/9yPupG6hZoTjPXdmIFrXKe11W\nyDGzBc65xFyfF4z1r2ZWA5jsnGuY0/MSExNdampqwOsRkfAyZ/UOBk1YwoZdB7m5ZQIPda5HyWKx\nXpcVMnwN/ECu0onPdPMKYFmgxhKRwq11nQpM7deOO1rXZOz89GZsX6/c5nVZYSeQc/gvmNlSM1sC\nXAj0D+BYIlLIxRWJ4V+XN2B87/MpXjSG29/6nv7vL2LX72rG5qugTOn4SlM6IuKLw8eOM/yr1Yz4\nZg2lT4vliW5nc2mjeMzM69I84fmUjohIoBSNiWZAx3p8el8bqpQ5jb7v/EDPMQvYqmZsOVLgi0jY\nOiu+FB/fez4Pd6lPys/baZ8dv1ocAAAI20lEQVQ8k/e+W69mbNlQ4ItIWIuJjqJXUm2m9mtHg/hS\nDJqwlB6vzWf9TjVjO5UCX0QKhRoVivPu3S159opGLNm4l05DUnh9tpqxZabAF5FCIyrKuLFFdaYP\naMf5tcvz1OQVXDVyLj9vVTM2UOCLSCEUX/o0Xrs1kaHXN2Xdzt+5dNgshn25iiPHIrsZmwJfRAol\nM6Nb06rMGJBE54bxJE//ma4vz2bxhj1el+YZBb6IFGrlSxTlpRvO4dVbEtmddoQrRszh2Sk/cvBI\n5DVjU+CLSETo0KAy0wckcd151RiV8gtdhqYwb81Or8sKKgW+iESMUsVi+b8rG/PO3S044eCGV7/l\nHx8vZd+ho16XFhQKfBGJOOfXrsAX/dpyV5uavPfdejomp/DVyq1elxVwCnwRiUhxRWJ49LIGTLi3\nNaVPi+WOt1J54L0f2HngsNelBYwCX0QiWtNqZfj0vjb0a1+XKUs302FwChMXbSqU7RkU+CIS8YrE\nRNGv/ZlMvq8t1cqexgPvLeKu0als3nvQ69L8SoEvIpKh3uklGd/7fB655CzmrNlBx+QU3pm/nhOF\npD2DAl9EJJOY6CjubleLqf3a0bBqaf7x8VJufO1b1u743evSCkyBLyKShYTyxXnn7hY8d2Ujlm/a\nR6chKYxKWcOx4+HbnkGBLyKSDTPj+ubVmT4gibZ1K/DslJVcNXIuK7fs87q0fFHgi4jk4vTSxXj1\nlkSG3XAOG3cf5LJhs0me/jOHj4VXewYFvoiID8yMrk2qMH1AEpc1jmfYl6u4bNhsFq7f7XVpPlPg\ni4jkQbniRRhy/Tm8cVsiBw4f46qRc3lq8grSjhzzurRcKfBFRPLhovqVmda/HT1aVOf12b/SaUgK\nc1bv8LqsHCnwRUTyqWSxWJ7u3oj3e7YkJiqKHq/NZ9D4Jew9GJrN2AoU+GZ2jZktN7MTZpZ4ymMP\nm9lqM/vJzDoVrEwRkdDVolZ5Pn+gLb2SavFB6gY6JM9k2vItXpf1FwU9wl8GXAmkZL7TzBoA1wNn\nA52BEWYWXcCxRERCVrHYaB7uchYT+7ShXPEi9ByzgL7vLGRHCDVjK1DgO+d+dM79lMVD3YD3nHOH\nnXO/AquB5gUZS0QkHDQ6ozSf3teGBzucybTlW2mfPJOPf9gYEs3YAjWHXxXYkOn2xoz7REQKvdjo\nKO67uC6f3d+GmhWK0//9xdz+1vds2uNtM7ZcA9/MZpjZsiy2bv4owMx6mlmqmaVu377dH7sUEQkJ\ndSuX5KN7zudflzVg/i+76Jg8kzHz1nrWjC0mtyc459rnY7+bgGqZbp+RcV9W+x8FjAJITEz0/jOP\niIgfRUcZd7SpSYcGlXl4wlL+OXE5ny7ezHNXNaJWxRJBrSVQUzqTgOvNrKiZ1QTqAt8FaCwRkZBX\nrVwcY+5szgtXN2blln10HjqLkd8EtxlbQZdlXmFmG4FWwGdmNhXAObcc+ABYAXwB9HHOhVfTCRER\nPzMzrk2sxowBSVxYryLPf7GS7iPmsOK34DRjs1A4c3xSYmKiS01N9boMEZGAc87x+bIt/GviMvak\nHWVQl/rc1bZWvvZlZgucc4m5PS/XOXwREfE/M+OSRvG0qlWepz/7kRrliwd8TAW+iIiHyhYvwn+u\nbRKUsdRLR0QkQijwRUQihAJfRCRCKPBFRCKEAl9EJEIo8EVEIoQCX0QkQijwRUQiREi1VjCz7cC6\nfP55BSC0v0E4e6rdG6rdG+FaeyjXneCcq5jbk0Iq8AvCzFJ96SURilS7N1S7N8K19nCtOzNN6YiI\nRAgFvohIhChMgT/K6wIKQLV7Q7V7I1xrD9e6/1Bo5vBFRCRnhekIX0REclAoAt/MOpvZT2a22swG\neV2Pr8ysmpl9bWYrzGy5mT3gdU15YWbRZvaDmU32upa8MLMyZvaRma00sx/NrJXXNfnKzPpn/Ley\nzMzeNbNiXteUHTN7w8y2mdmyTPeVM7PpZrYq42dZL2vMTja1v5jx38wSM/vYzMp4WWN+hH3gm1k0\nMBzoAjQAbjCzBt5W5bNjwIPOuQZAS6BPGNUO8ADwo9dF5MNQ4AvnXH2gCWHyGsysKnA/kOicawhE\nA9d7W1WO3gI6n3LfIOBL51xd4MuM26HoLf5a+3SgoXOuMfAz8HCwiyqosA98oDmw2jn3i3PuCPAe\n0M3jmnzinNvsnFuY8ft+0oOnqrdV+cbMzgAuBV7zupa8MLPSQDvgdQDn3BHn3B5vq8qTGOA0M4sB\n4oDfPK4nW865FGDXKXd3A0Zn/D4a6B7UonyUVe3OuWnOuWMZN78Fzgh6YQVUGAK/KrAh0+2NhElo\nZmZmNYBzgPneVuKzIcBDwAmvC8mjmsB24M2M6ajXzCzwXybqB865TcC/gfXAZmCvc26at1XlWWXn\n3OaM37cAlb0spgDuAD73uoi8KgyBH/bMrAQwHujnnNvndT25MbPLgG3OuQVe15IPMcC5wEjn3DnA\n74TutMKfZMx3dyP9TasKUNzMbvK2qvxz6UsEw26ZoJk9Qvp07Diva8mrwhD4m4BqmW6fkXFfWDCz\nWNLDfpxzboLX9fioNdDVzNaSPoV2kZmN9bYkn20ENjrnTn6S+oj0N4Bw0B741Tm33Tl3FJgAnO9x\nTXm11cziATJ+bvO4njwxs9uAy4AeLgzXtBeGwP8eqGtmNc2sCOknsSZ5XJNPzMxIn0v+0TmX7HU9\nvnLOPeycO8M5V4P0f++vnHNhcaTpnNsCbDCzehl3XQys8LCkvFgPtDSzuIz/di4mTE44ZzIJuDXj\n91uBiR7Wkidm1pn0acyuzrk0r+vJj7AP/IyTKH2BqaT/x/+Bc265t1X5rDVwM+lHyIsytku8LioC\n3AeMM7MlQFPgWY/r8UnGp5KPgIXAUtL//w3Zqz/N7F1gHlDPzDaa2Z3Ac0AHM1tF+ieW57ysMTvZ\n1P4yUBKYnvH/6iueFpkPutJWRCRChP0RvoiI+EaBLyISIRT4IiIRQoEvIhIhFPgiIhFCgS8iEiEU\n+CIiEUKBLyISIf4f4BcP5y0Ex6MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbad3a47310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "domain =  np.linspace(0,13,100)\n",
    "h_x = -(w[0,0]/w[0,1])*domain - (w[0,2]/w[0,1])\n",
    "\n",
    "plt.plot(v_class1[:,0],v_class1[:,1],'ro',v_class2[:,0],v_class2[:,1],'bo')\n",
    "plt.plot(domain,h_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
